{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sIbQph5zREKn"
      },
      "outputs": [],
      "source": [
        "### sklearn code for Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.preprocessing import (StandardScaler, LabelEncoder,\n",
        "                                 MultiLabelBinarizer, OneHotEncoder,\n",
        "                                 FunctionTransformer)\n",
        "from sklearn.model_selection import (train_test_split, GridSearchCV, RandomizedSearchCV)\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import (MultinomialNB, GaussianNB, BernoulliNB)\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "yqkxJAvJROuM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('sample_data/final_final_fr_fr.csv')\n",
        "\n",
        "# Print unique values for each column\n",
        "for column in df.columns:\n",
        "    print(f\"Unique values in {column}: {df[column].unique()}\")"
      ],
      "metadata": {
        "id": "c0RJWk9bRTgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### same as Karyna's implementation (for data organization) from sklearn_logistic_regression\n",
        "# Define vectorized_one_hot function\n",
        "def vectorized_one_hot(answers, options, attribute_to_index):\n",
        "    num_samples = len(answers)\n",
        "    num_attributes = len(options)\n",
        "    one_hot_matrix = np.zeros((num_samples, num_attributes))\n",
        "\n",
        "    for i, ans_list in enumerate(answers):\n",
        "        for ans in ans_list:\n",
        "            ans = ans.strip()\n",
        "            if ans in attribute_to_index:\n",
        "                one_hot_matrix[i, attribute_to_index[ans]] = 1\n",
        "    return one_hot_matrix\n",
        "\n",
        "# Question definitions\n",
        "q1 = \"Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)\"\n",
        "q2 = \"Q2: How many ingredients would you expect this food item to contain?\"\n",
        "q3 = \"Q3: In what setting would you expect this food to be served? Please check all that apply\"\n",
        "q4 = \"Q4: How much would you expect to pay for one serving of this food item?\"\n",
        "q5 = \"Q5: What movie do you think of when thinking of this food item?\"\n",
        "q6 = \"Q6: What drink would you pair with this food item?\"\n",
        "q7 = \"Q7: When you think about this food item, who does it remind you of?\"\n",
        "q8 = \"Q8: How much hot sauce would you add to this food item?\"\n",
        "t = \"Label\"\n",
        "\n",
        "# Data cleaning\n",
        "df[q2] = df[q2].replace(\"none\", '0').astype(str)\n",
        "df[q4] = df[q4].replace(\"none\", '0').astype(str)\n",
        "\n",
        "# Define options for categorical columns\n",
        "q1_options = [1,2,3,4,5]\n",
        "q3_options = ['none','Week day lunch','Week day dinner','Weekend lunch','Weekend dinner','At a party', 'Late night snack']\n",
        "q7_options = ['Parents','Siblings','Friends', 'Teachers', 'Strangers' , 'none']\n",
        "q8_options = ['I will have some of this food item with my hot sauce', 'A lot (hot)', 'A moderate amount (medium)', 'A little (mild)', 'none']\n",
        "\n",
        "# Create attribute to index mappings\n",
        "q3_attribute_to_index = {attr: idx for idx, attr in enumerate(q3_options)}\n",
        "q7_attribute_to_index = {attr: idx for idx, attr in enumerate(q7_options)}\n",
        "q8_attribute_to_index = {attr: idx for idx, attr in enumerate(q8_options)}\n",
        "\n",
        "# Manual feature engineering approach\n",
        "df[q2] = pd.to_numeric(df[q2])\n",
        "df[q1] = pd.to_numeric(df[q1])\n",
        "df[q4] = pd.to_numeric(df[q4])\n",
        "\n",
        "numerical_features = df[[q1, q2, q4]].values\n",
        "\n",
        "# Multi-hot encoding\n",
        "q3_hot = vectorized_one_hot([ans.split(\",\") for ans in df[q3].astype(str).tolist()], q3_options, q3_attribute_to_index)\n",
        "q7_hot = vectorized_one_hot([ans.split(\",\") for ans in df[q7].astype(str).tolist()], q7_options, q7_attribute_to_index)\n",
        "q8_hot = vectorized_one_hot([ans.split(\",\") for ans in df[q8].astype(str).tolist()], q8_options, q8_attribute_to_index)\n",
        "\n",
        "# Frequency encoding\n",
        "q5_encoded = df[q5].map(df[q5].value_counts(normalize=True)).to_frame()\n",
        "q6_encoded = df[q6].map(df[q6].value_counts(normalize=True)).to_frame()\n",
        "\n",
        "# Combine features\n",
        "X = np.hstack([\n",
        "    numerical_features,\n",
        "    q3_hot,\n",
        "    q7_hot,\n",
        "    q8_hot,\n",
        "    q5_encoded,\n",
        "    q6_encoded\n",
        "])\n",
        "\n",
        "# Sparse version\n",
        "X_sparse = sparse.hstack([\n",
        "    sparse.csr_matrix(numerical_features),\n",
        "    sparse.csr_matrix(q3_hot),\n",
        "    sparse.csr_matrix(q7_hot),\n",
        "    sparse.csr_matrix(q8_hot),\n",
        "    sparse.csr_matrix(q5_encoded),\n",
        "    sparse.csr_matrix(q6_encoded)\n",
        "])\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[\"Label\"])\n",
        "\n",
        "# train-validatin-test split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_sparse, y, test_size=0.2, random_state=42)  # 80% train, 20% temp (will use temp for val and test)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 10% val, 10% test"
      ],
      "metadata": {
        "id": "R0bErg1JR5eQ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### new stuff for naive bayes\n",
        "nb_model = MultinomialNB()\n",
        "# nb_model = GaussianNB()       # these two dont actually work well for our data\n",
        "# nb_model = BernoulliNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate Performance -- BEFORE ANY TUNING\n",
        "print(\"BEFORE TUNING\")\n",
        "train_accuracy = accuracy_score(y_train, nb_model.predict(X_train))\n",
        "val_accuracy = accuracy_score(y_val, nb_model.predict(X_val))\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
        "print(f\"Testing Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\") # 0= Pizza, 1= Shawarma, 2= Sushi\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15Lx6sbiryaw",
        "outputId": "929e4090-55e6-49e3-930b-5ef7a7d2714f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE TUNING\n",
            "Training Accuracy: 0.73\n",
            "Validation Accuracy: 0.71\n",
            "Testing Accuracy: 0.73\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.72      0.70        43\n",
            "           1       0.84      0.80      0.82        70\n",
            "           2       0.62      0.63      0.63        52\n",
            "\n",
            "    accuracy                           0.73       165\n",
            "   macro avg       0.72      0.72      0.72       165\n",
            "weighted avg       0.73      0.73      0.73       165\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[31  1 11]\n",
            " [ 5 56  9]\n",
            " [ 9 10 33]]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Tuning parameters to try to get better accuracy\n",
        "## attempt 1 to tune parameters: use grid search\n",
        "#  by including 0 as option for alpha, we are allowing MLE as an option (alpha>0 --> MAP)\n",
        "param_grid = {'alpha': [0, 0.01, 0.1, 1.0, 10.0],\n",
        "              'fit_prior': [True, False]}\n",
        "\n",
        "grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "grid_params = grid_search.best_params_\n",
        "\n",
        "new_grid_model = MultinomialNB(\n",
        "    alpha = grid_params['alpha'],\n",
        "    fit_prior = grid_params['fit_prior']\n",
        ")\n",
        "new_grid_model.fit(X_train, y_train)\n",
        "# new_grid_y_pred = new_grid_model.predict(X_test)                                                    NO LONGER NEED HERE WITH NEW ORGANIZATION\n",
        "\n",
        "# Evaluate on validation set before testing -- USING GRID SEARCH\n",
        "grid_val_accuracy = accuracy_score(y_val, new_grid_model.predict(X_val))  # Validation set accuracy\n",
        "print(f\"Validation Accuracy after Grid Search: {grid_val_accuracy:.2f}\")\n",
        "\n",
        "# # Evaluate Performance -- USING GRID SEARCH\n",
        "# print(\"TUNING USING GRID SEARCH\")\n",
        "# accuracy = accuracy_score(y_test, new_grid_y_pred) # now using the new y_pred\n",
        "# print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# print(\"\\nClassification Report:\") # 0= Pizza, 1= Shawarma, 2= Sushi\n",
        "# print(classification_report(y_test, new_grid_y_pred))\n",
        "\n",
        "# print(\"\\nConfusion Matrix:\")\n",
        "# print(confusion_matrix(y_test, new_grid_y_pred))\n",
        "# print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "## attempt 2 to tune parameters: use random search\n",
        "# Define parameter distribution -- by including 0 as option for alpha, we are allowing MLE as an option (alpha>0 --> MAP)\n",
        "param_dist = {\n",
        "    'alpha': [0, 0.01, 0.1, 1.0, 10.0],\n",
        "    'fit_prior': [True, False]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(MultinomialNB(), param_distributions=param_dist, n_iter=5, cv=10, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best parameters:\", random_search.best_params_)\n",
        "rand_params = random_search.best_params_\n",
        "\n",
        "# Create a model with the best parameters\n",
        "new_rand_model = MultinomialNB(\n",
        "    alpha=rand_params['alpha'],\n",
        "    fit_prior=rand_params['fit_prior']\n",
        ")\n",
        "\n",
        "# Train on full training data\n",
        "new_rand_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Evaluate on validation set before testing -- USING RANDOM SEARCH\n",
        "rand_val_accuracy = accuracy_score(y_val, new_rand_model.predict(X_val))  # Validation set accuracy\n",
        "print(f\"Validation Accuracy after Random Search: {rand_val_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDxcpIUXr7Gx",
        "outputId": "955e997b-705d-4640-b3b7-e25f8537ccc3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'alpha': 0.1, 'fit_prior': True}\n",
            "Validation Accuracy after Grid Search: 0.71\n",
            "Best parameters: {'fit_prior': False, 'alpha': 0}\n",
            "Validation Accuracy after Random Search: 0.70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py:898: RuntimeWarning: divide by zero encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py:898: RuntimeWarning: divide by zero encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### choosing the best tuned parameters and comparing against the un-tuned parameters\n",
        "# figure out if grid search or random search did a better job tuning\n",
        "if grid_val_accuracy > rand_val_accuracy:\n",
        "    print(\"BETTER TUNING: GRID SEARCH\")\n",
        "    tuned_params = grid_params\n",
        "    better_model = new_grid_model\n",
        "else:\n",
        "    print(\"BETTER TUNING: RANDOM SEARCH\")\n",
        "    tuned_params = rand_params\n",
        "    better_model = new_rand_model\n",
        "\n",
        "tuned_y_pred = new_grid_model.predict(X_test)\n",
        "\n",
        "# Evaluate Performance -- USING BETTER (of 2) TUNING\n",
        "accuracy = accuracy_score(y_test, tuned_y_pred) # now using the new y_pred\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\") # 0= Pizza, 1= Shawarma, 2= Sushi\n",
        "print(classification_report(y_test, tuned_y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, tuned_y_pred))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Evaluate Performance -- RECALL BEFORE ANY TUNING\n",
        "print(\"BEFORE TUNING\")\n",
        "train_accuracy = accuracy_score(y_train, nb_model.predict(X_train))\n",
        "val_accuracy = accuracy_score(y_val, nb_model.predict(X_val))\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
        "print(f\"Testing Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\") # 0= Pizza, 1= Shawarma, 2= Sushi\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxZzFzXhr_8r",
        "outputId": "c18ea295-c5f6-4948-df09-c90b5a961bcf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BETTER TUNING: GRID SEARCH\n",
            "Accuracy: 0.72\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.72      0.69        43\n",
            "           1       0.84      0.80      0.82        70\n",
            "           2       0.61      0.60      0.60        52\n",
            "\n",
            "    accuracy                           0.72       165\n",
            "   macro avg       0.70      0.71      0.70       165\n",
            "weighted avg       0.72      0.72      0.72       165\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[31  1 11]\n",
            " [ 5 56  9]\n",
            " [11 10 31]]\n",
            "\n",
            "\n",
            "\n",
            "BEFORE TUNING\n",
            "Training Accuracy: 0.73\n",
            "Validation Accuracy: 0.71\n",
            "Testing Accuracy: 0.73\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.72      0.70        43\n",
            "           1       0.84      0.80      0.82        70\n",
            "           2       0.62      0.63      0.63        52\n",
            "\n",
            "    accuracy                           0.73       165\n",
            "   macro avg       0.72      0.72      0.72       165\n",
            "weighted avg       0.73      0.73      0.73       165\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[31  1 11]\n",
            " [ 5 56  9]\n",
            " [ 9 10 33]]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}