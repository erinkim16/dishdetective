{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erinkim16/dishdetective/blob/main/erin_finals_merged.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlxskmAin2Px",
        "outputId": "eca7cb57-216f-482b-b669-0579ec725856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=2f7220255f278599dd783ccc8435c6ce853b09af6dca49ae1dfe20193b9212b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install word2number\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from word2number import w2n\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "A6ULwUzCOM-u",
        "outputId": "a9f05b26-2d06-40ff-cda5-6deb6a2f6eaf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmDHwNuMnfKp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/311/cleaned_data_combined_modified.csv')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UfHZeg_pEha"
      },
      "source": [
        "How to parse \"How many ingredients\" Q2\n",
        "\n",
        "*  take average\n",
        "*  take first number\n",
        "* take last number\n",
        "* count how many spaces in between the words or commas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUcxZ3h_pD8F"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary to map month abbreviations to numbers\n",
        "month_map = {\n",
        "    'Jan': '1',\n",
        "    'Feb': '2',\n",
        "    'Mar': '3',\n",
        "    'Apr': '4',\n",
        "    'May': '5',\n",
        "    'Jun': '6',\n",
        "    'Jul': '7',\n",
        "    'Aug': '8',\n",
        "    'Sep': '9',\n",
        "    'Oct': '10',\n",
        "    'Nov': '11',\n",
        "    'Dec': '12'\n",
        "}\n",
        "\n",
        "# Function to convert date format\n",
        "def convert_date_format(date_str):\n",
        "    if isinstance(date_str, str) and '-' in date_str:\n",
        "        try:\n",
        "            day, month_abbr = date_str.split('-')\n",
        "            month_num = month_map.get(month_abbr, month_abbr)\n",
        "            return f\"{int(month_num)}-{int(day)}\"\n",
        "        except ValueError:\n",
        "           return date_str\n",
        "    return date_str\n",
        "\n",
        "# Apply the function to the Q2 column\n",
        "df['Q2: How many ingredients would you expect this food item to contain?'] = df['Q2: How many ingredients would you expect this food item to contain?'].apply(convert_date_format)\n",
        "\n",
        "# Function to take the average if there's a range(either with '-' or 'to')\n",
        "def average_of_range(text):\n",
        "  if isinstance(text, str):\n",
        "    range_match = re.search(r'(\\d+)[\\s-]+(\\d+)', text)\n",
        "    range_match2 = re.search(r'(\\d+)\\s* to \\s*(\\d+)', text)\n",
        "\n",
        "    if range_match is not None:\n",
        "      num1, num2 = int(range_match.group(1)), int(range_match.group(2))\n",
        "      return int((num1 + num2) / 2 )# Take the average of the range\n",
        "    elif range_match2 is not None:\n",
        "      num1, num2 = int(range_match2.group(1)), int(range_match2.group(2))\n",
        "      return int((num1 + num2) / 2)\n",
        "    else:\n",
        "      return text\n",
        "\n",
        "# Apply the function to the Q2 column\n",
        "df['Q2: How many ingredients would you expect this food item to contain?'] = df['Q2: How many ingredients would you expect this food item to contain?'].apply(average_of_range)\n",
        "\n",
        "\n",
        "# Function to extract numerical values in data entry\n",
        "def extract_number(text):\n",
        "  # check if entry is string\n",
        "  if isinstance(text, str):\n",
        "    # search for any digits in the text\n",
        "    # note, this is called after all previous functions, which should indicate high likelihood\n",
        "    # that what's left is the answer value with only v/ small margin of error\n",
        "    num = re.search(r'(\\d+)', text)\n",
        "\n",
        "    # if found, return as int\n",
        "    if num is not None:\n",
        "      return int(num.group(1))\n",
        "\n",
        "    # if no match, return original entry\n",
        "    else:\n",
        "      return text\n",
        "  # if not, return original entry\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "# Apply the function to the Q2 column\n",
        "df['Q2: How many ingredients would you expect this food item to contain?'] = df['Q2: How many ingredients would you expect this food item to contain?'].apply(extract_number)\n",
        "\n",
        "\n",
        "def convert_wrds(text):\n",
        "    if isinstance(text, str):\n",
        "        # Regex to find hyphenated numbers (e.g., \"twenty-two\", \"one-hundred-three\")\n",
        "        hyphenated_pattern = r'\\b(?:one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion)(?:-\\s?(?:one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion))+\\b'\n",
        "\n",
        "        # Regex to find single (non-hyphenated) number words\n",
        "        single_pattern = r'\\b(one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion)\\b'\n",
        "\n",
        "        # Look for a hyphenated number first\n",
        "        hyphenated_match = re.search(hyphenated_pattern, text, re.IGNORECASE)\n",
        "        if hyphenated_match:\n",
        "            try:\n",
        "                return w2n.word_to_num(hyphenated_match.group(0))  # Convert full hyphenated number\n",
        "            except ValueError:\n",
        "                return hyphenated_match.group(0)  # Return as text if conversion fails\n",
        "\n",
        "        # If no hyphenated number, look for a single non-hyphenated number\n",
        "        single_match = re.search(single_pattern, text, re.IGNORECASE)\n",
        "        if single_match:\n",
        "            try:\n",
        "                return w2n.word_to_num(single_match.group(1))  # Convert first non-hyphenated number\n",
        "            except ValueError:\n",
        "                return single_match.group(1)  # Return as text if conversion fails\n",
        "\n",
        "    return text  # Return original text if no numbers found\n",
        "\n",
        "# Apply function to the Q2 column\n",
        "df['Q2: How many ingredients would you expect this food item to contain?'] = df[\n",
        "    'Q2: How many ingredients would you expect this food item to contain?'\n",
        "].apply(convert_wrds)\n",
        "\n",
        "# Function to count the number of ingredients if responses are comma-separated or use '*'\n",
        "def count_ingredients(text):\n",
        "    if isinstance(text, str):  # Ensure it's a string\n",
        "        if ',' in text:  # Count comma-separated items\n",
        "            return len([item.strip() for item in text.split(',') if item.strip()])\n",
        "        elif '* ' in text:  # Count bullet-pointed items\n",
        "            return len([item.strip() for item in text.split('* ') if item.strip()])\n",
        "    return text  # If it's a single ingredient or doesn't match conditions, return 1\n",
        "\n",
        "# Apply the function to the Q2 column\n",
        "df['Q2: How many ingredients would you expect this food item to contain?'] = df[\n",
        "    'Q2: How many ingredients would you expect this food item to contain?'\n",
        "].apply(count_ingredients)\n",
        "\n",
        "\n",
        "# Function to identify likely 'outliers' (ie. stories/anecdotes, 'i don't knows', etc.)\n",
        "def outlier_rest(text):\n",
        "  # check if entry is string\n",
        "  if isinstance(text, str):\n",
        "    # search for any text characters\n",
        "    is_match = re.search(r'(\\s+)', text)\n",
        "    # if found, return 'None' as a string to identify an outlier in the data\n",
        "    if is_match is not None:\n",
        "      return 'None'\n",
        "\n",
        "    # if not, return original entry\n",
        "    else:\n",
        "      return text\n",
        "  # if not, return original\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "df['Q2: How many ingredients would you expect this food item to contain?'] = df[\n",
        "    'Q2: How many ingredients would you expect this food item to contain?'\n",
        "].apply(outlier_rest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T7gG52gfDpD"
      },
      "source": [
        "How to parse \"How much would you pay\" Q4\n",
        "*   first number seen\n",
        "*   average of many numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRZxPMmVfK9F"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Function to take the average if there's a range(either with '-' or 'to')\n",
        "def q4_range(text):\n",
        "  # make sure data entry is a string\n",
        "  if isinstance(text, str):\n",
        "\n",
        "    # case where there's a range and no dollar sign (eg. 2-5)\n",
        "    no_sign = re.search(r'(\\d+)\\s*(?:-|to|up to)\\s*(\\d+)', text)\n",
        "\n",
        "    # case where there's a range w/ a dollar sign at the front (eg. 5)\n",
        "    sign_front = re.search(r'(\\d+)\\s*(?:-|to|up to)\\s*$(\\d+)', text)\n",
        "\n",
        "    # case where there's a range w/ a dollar sign at the back (eg. 2)\n",
        "    sign_back = re.search(r'(\\d+)$\\s*(?:-|to|up to)\\s*(\\d+)', text)\n",
        "\n",
        "    # if one is not None, ie a match was found, extract #s and take average\n",
        "    if no_sign is not None:\n",
        "      num1, num2 = int(no_sign.group(1)), int(no_sign.group(2))\n",
        "      return (num1 + num2) / 2\n",
        "\n",
        "    elif sign_front is not None:\n",
        "      num1, num2 = int(sign_front.group(1)), int(sign_front.group(2))\n",
        "      return (num1 + num2) / 2\n",
        "\n",
        "    elif sign_back is not None:\n",
        "      num1, num2 = int(sign_back.group(1)), int(sign_back.group(2))\n",
        "      return (num1 + num2) / 2\n",
        "\n",
        "    # if no match found, return original data entry\n",
        "    else:\n",
        "      return text\n",
        "  # if not string, return original entry\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "# Function to find float money values and return as floats\n",
        "def floats(text):\n",
        "  # check if data entry is a string\n",
        "  if isinstance(text, str):\n",
        "    # find values in full 'monetary' form(ie. 12.00 instead of 12, or 12.99)\n",
        "    float_vars = re.search(r'(\\d+\\.\\d{2})', text)\n",
        "\n",
        "    # if not None, ie match found, return the value in float format\n",
        "    if float_vars is not None:\n",
        "      return float(float_vars.group(1))\n",
        "\n",
        "    # if no match found, return original entry\n",
        "    else:\n",
        "      return text\n",
        "  # if not string, return original entry\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "# Function to find values with written 'money' indications (ie. 12 dollars)\n",
        "def dollars(text):\n",
        "  # check if entry is a string\n",
        "  if isinstance(text, str):\n",
        "    # search for variations of 'money-indicative' words that have a number directly preceeding it\n",
        "    dollar_vars = re.search(r'(\\d+)\\s*(dollars|dollar|dollor|bucks|buck)', text, re.IGNORECASE)\n",
        "    currencies = re.search(r'(\\d+)\\s*(CAD|USD|Canadian)', text, re.IGNORECASE)\n",
        "\n",
        "    # if match for either found, return float value of that number\n",
        "    if dollar_vars is not None:\n",
        "      return float(dollar_vars.group(1))\n",
        "    elif currencies is not None:\n",
        "      return float(currencies.group(1))\n",
        "\n",
        "    # if no match found, return original\n",
        "    else:\n",
        "      return text\n",
        "  # if not, return original entry\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "# Function to find values directly preceding or following a '$' symbol\n",
        "def dollar_sign(text):\n",
        "  # make sure entry is a string\n",
        "  if isinstance(text, str):\n",
        "\n",
        "    # search for numbers either before or after '$'\n",
        "    # note, this is called after q4_range to not misselect 5 as just 4\n",
        "    sign_first = re.search(r'$(\\d+)', text)\n",
        "    sign_after = re.search(r'(\\d+)\\s*$', text)\n",
        "\n",
        "    # if found, return value as float\n",
        "    if sign_first is not None:\n",
        "      return float(sign_first.group(1))\n",
        "    elif sign_after is not None:\n",
        "      return float(sign_after.group(1))\n",
        "\n",
        "    # if no match found, return original entry\n",
        "    else:\n",
        "      return text\n",
        "  # if not, return original entry\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "# Function to extract numerical values in data entry\n",
        "def extract_number(text):\n",
        "  # check if entry is string\n",
        "  if isinstance(text, str):\n",
        "    # search for any digits in the text\n",
        "    # note, this is called after all previous functions, which should indicate high likelihood\n",
        "    # that what's left is the answer value with only v/ small margin of error\n",
        "    num = re.search(r'(\\d+)', text)\n",
        "\n",
        "    # if found, return as float\n",
        "    if num is not None:\n",
        "      return float(num.group(1))\n",
        "\n",
        "    # if no match, return original entry\n",
        "    else:\n",
        "      return text\n",
        "  # if not, return original entry\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "# Function to find written numbercal values and convert into proper form\n",
        "\n",
        "def convert_wrds(text):\n",
        "    if isinstance(text, str):\n",
        "        # Regex to find hyphenated numbers (e.g., \"twenty-two\", \"one-hundred-three\")\n",
        "        hyphenated_pattern = r'\\b(?:one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion)(?:-\\s?(?:one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion))+\\b'\n",
        "\n",
        "        # Regex to find single (non-hyphenated) number words\n",
        "        single_pattern = r'\\b(one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion)\\b'\n",
        "\n",
        "        # Look for a hyphenated number first\n",
        "        hyphenated_match = re.search(hyphenated_pattern, text, re.IGNORECASE)\n",
        "        if hyphenated_match:\n",
        "            try:\n",
        "                return w2n.word_to_num(hyphenated_match.group(0))  # Convert full hyphenated number\n",
        "            except ValueError:\n",
        "                return hyphenated_match.group(0)  # Return as text if conversion fails\n",
        "\n",
        "        # If no hyphenated number, look for a single non-hyphenated number\n",
        "        single_match = re.search(single_pattern, text, re.IGNORECASE)\n",
        "        if single_match:\n",
        "            try:\n",
        "                return w2n.word_to_num(single_match.group(1))  # Convert first non-hyphenated number\n",
        "            except ValueError:\n",
        "                return single_match.group(1)  # Return as text if conversion fails\n",
        "\n",
        "    return text  # Return original text if no numbers found\n",
        "\n",
        "# Apply function to the Q2 column\n",
        "df['Q2: How many ingredients would you expect this food item to contain?'] = df[\n",
        "    'Q2: How many ingredients would you expect this food item to contain?'\n",
        "].apply(convert_wrds)\n",
        "\n",
        "# Function to identify likely 'outliers' (ie. stories/anecdotes, 'i don't knows', etc.)\n",
        "def outlier_rest(text):\n",
        "  # check if entry is string\n",
        "  if isinstance(text, str):\n",
        "    # search for any text characters\n",
        "    is_match = re.search(r'(\\s+)', text)\n",
        "    # if found, return 'None' as a string to identify an outlier in the data\n",
        "    if is_match is not None:\n",
        "      return 'None'\n",
        "\n",
        "    # if not, return original entry\n",
        "    else:\n",
        "      return text\n",
        "  # if not, return original\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "# Applying all functions to the Q4 column\n",
        "# Important note! ORDER MATTERS! Specific order to identify cases, and not *mis*-identify cases with only very small margin of error\n",
        "\n",
        "# Apply the date function to the Q4 column\n",
        "df['Q4: How much would you expect to pay for one serving of this food item?'] = df['Q4: How much would you expect to pay for one serving of this food item?'].apply(convert_date_format)\n",
        "\n",
        "# Apply the q4_range function to the Q4 column\n",
        "df['Q4: How much would you expect to pay for one serving of this food item?'] = df['Q4: How much would you expect to pay for one serving of this food item?'].apply(q4_range)\n",
        "\n",
        "# Apply the floats function to the Q4 column\n",
        "df['Q4: How much would you expect to pay for one serving of this food item?'] = df['Q4: How much would you expect to pay for one serving of this food item?'].apply(floats)\n",
        "\n",
        "# Apply the dollars function to the Q4 column\n",
        "df['Q4: How much would you expect to pay for one serving of this food item?'] = df['Q4: How much would you expect to pay for one serving of this food item?'].apply(dollars)\n",
        "\n",
        "# Apply the dollar_sign function to the Q4 column\n",
        "df['Q4: How much would you expect to pay for one serving of this food item?'] = df['Q4: How much would you expect to pay for one serving of this food item?'].apply(dollar_sign)\n",
        "\n",
        "# Apply the extract number function to the Q4 column\n",
        "df['Q4: How much would you expect to pay for one serving of this food item?'] = df['Q4: How much would you expect to pay for one serving of this food item?'].apply(extract_number)\n",
        "\n",
        "# Apply the convert words function to the Q4 column\n",
        "df['Q4: How much would you expect to pay for one serving of this food item?'] = df['Q4: How much would you expect to pay for one serving of this food item?'].apply(convert_wrds)\n",
        "\n",
        "# Apply the outlier_rest function to the Q4 column\n",
        "df['Q4: How much would you expect to pay for one serving of this food item?'] = df['Q4: How much would you expect to pay for one serving of this food item?'].apply(outlier_rest)\n",
        "\n",
        "df.to_csv('\\content\\test.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDhRoqOwpiMK"
      },
      "source": [
        "Q6: can parse the data by classifying similar ones, like coke, soda, sprite, wine, ...\n",
        "\n",
        "\n",
        "*   by whcihever one we find first from sort data set alphabetically\n",
        "* find all matches from data set and randomly choose it\n",
        "* find all matches from data set and pick which one comes earliest (first)\n",
        "*   by whichever one we find first from sort data set by frequency of dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqQPjUW3xpFa"
      },
      "outputs": [],
      "source": [
        "# pandas, numpy, download stuff\n",
        "\n",
        "######### drinks ############\n",
        "\n",
        "# lowercase everything\n",
        "\n",
        "# find the median word length to decide how to make split\n",
        "# of yap vs actual movie name / drink name\n",
        "\n",
        "# ex: only 2 words length is enough to describe most drinks, only use that as\n",
        "# \"data set \" comparaison\n",
        "\n",
        "# group together based on persoal thought n how generalizable it is\n",
        "# using a dataset , soda~softdrink~pop same,\n",
        "\n",
        "# sort \"data set\" popularity (counts)\n",
        "\n",
        "# for long answers for drinks, to match with the \"data set\"\n",
        "# and take first match\n",
        "\n",
        "# keep track of any data that couldn't be matched\n",
        "# will see if we can label it as 'None' / noise\n",
        "# or if we need to include more drinks in our 'dataset'\n",
        "\n",
        "# pandas, numpy, download stuff\n",
        "\n",
        "######## movie #############\n",
        "\n",
        "# remove everyting after :......\n",
        "# if there's a number , like shrek 2, remove the number 2, but if its 16 candles,\n",
        "# don't remove cos its likely part of the title\n",
        "\n",
        "# lowercase thing\n",
        "\n",
        "# divide based on median, so we divide the yapping rants vs actual titles (assuming its likely chances)\n",
        "\n",
        "# research to try and find data set with all movies ever\n",
        "\n",
        "# For those that couldn't find a movie and was too big, first try to find the first actual movie name\n",
        "# that shows up, otherwise label it as 'None'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8ipNXvV9fW6"
      },
      "source": [
        "Q5 Movie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmY6iowBOmtM"
      },
      "outputs": [],
      "source": [
        "# pandas, numpy, download stuff\n",
        "\n",
        "######### drinks ############\n",
        "\n",
        "# lowercase everything\n",
        "\n",
        "# find the median word length to decide how to make split\n",
        "# of yap vs actual movie name / drink name\n",
        "\n",
        "# ex: only 2 words length is enough to describe most drinks, only use that as\n",
        "# \"data set \" comparaison\n",
        "\n",
        "# group together based on persoal thought n how generalizable it is\n",
        "# using a dataset , soda~softdrink~pop same,\n",
        "\n",
        "# sort \"data set\" popularity (counts)\n",
        "\n",
        "# for long answers for drinks, to match with the \"data set\"\n",
        "# and take first match\n",
        "\n",
        "# keep track of any data that couldn't be matched\n",
        "# will see if we can label it as 'None' / noise\n",
        "# or if we need to include more drinks in our 'dataset'\n",
        "\n",
        "######## movie #############\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxl_gjjKOmtM"
      },
      "source": [
        "Q5 Movie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv7M6z8LOmtN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUWpfvN-O_9g"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\"\"\" now look at cluster and fine tune trainign set \"\"\"\n",
        "\n",
        "\n",
        "\"\"\" game plan:\n",
        "group up (none): cluser id  254, 4 255\t258 277 84 307 44   216 327 150  304 20\n",
        "sort each group by length, then the final label is the median one in the lsit\n",
        "\n",
        "\n",
        "\n",
        "get a list of all the labels and thier group id, do classification with the logner sentences\n",
        "parsing the longer sentences:\n",
        "get rid of common stop words, use the outlier words if it's in it to consider it None, if it matches with none from our list even after parsing mkae it a\n",
        "new category, now go thru entire new longer sentence df nad see if any are similar enough to merge\n",
        "\n",
        "last, merge back using the original id's\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "### for bigger sentencees\n",
        "\n",
        "\n",
        "# Define outlier phrases\n",
        "outlier_phrases = [\n",
        "    \"come to mind\",\n",
        "    \"think of\", \"nothing comes\", \"comes to\", \"to mind\", \"come to\",\n",
        "    \"i dont\", \"none i\",\n",
        "    \"i think\", \"think of\", \"none\", \"not sure\", \"unsure\", \"None\",\n",
        "    \"idk\", \"nothing\", \"not \", \"any movie\", \"even though\"\n",
        "]\n",
        "\n",
        "# Function to check if a response is an outlier\n",
        "def is_outlier(response):\n",
        "    # Check if the response is fully blank or empty\n",
        "    if not response.strip():  # Removes whitespace and checks if empty\n",
        "        return True\n",
        "    response_lower = response.lower()  # Case-insensitive matching\n",
        "    for phrase in outlier_phrases:\n",
        "        if phrase in response_lower:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Separate outliers and \"no\" responses\n",
        "outliers = []\n",
        "no_responses = []\n",
        "updated_clusters = {}\n",
        "\n",
        "for cluster_id, group in clusters.items():\n",
        "    updated_group = []\n",
        "    for response in group:\n",
        "        if response.strip().lower() == \"no\":  # Check if response is exactly \"no\"\n",
        "            no_responses.append(response)  # Add to \"no\" responses\n",
        "        elif is_outlier(response):\n",
        "            outliers.append(response)  # Add to general outliers\n",
        "        else:\n",
        "            updated_group.append(response)  # Keep in the original cluster\n",
        "    if updated_group:  # Only add non-empty clusters\n",
        "        updated_clusters[cluster_id] = updated_group\n",
        "\n",
        "# Add the \"no\" responses and outliers as separate clusters\n",
        "if no_responses or outliers:\n",
        "    updated_clusters['outliers'] = no_responses + outliers\n",
        "\n",
        "# Convert updated clusters into a DataFrame\n",
        "cluster_data = []\n",
        "for cluster_id, group in updated_clusters.items():\n",
        "    cluster_data.append({\n",
        "        'Cluster ID': cluster_id,\n",
        "        'Responses': ', '.join(group),  # Combine responses into a single string\n",
        "        'Number of Responses': len(group)  # Add count of responses in the cluster\n",
        "    })\n",
        "\n",
        "# Create the DataFrame\n",
        "clusters_df = pd.DataFrame(cluster_data)\n",
        "\n",
        "clusters_df = clusters_df.sort_values(by='Number of Responses', ascending=False)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "clusters_df.to_csv('/content/clusters_output_v2.csv', index=False)\n",
        "\n",
        "# Output the DataFrame\n",
        "print(clusters_df)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LX5A3EpP5RP_"
      },
      "outputs": [],
      "source": [
        "#### cleaning Q6: What drink would you pair with this food item? ####\n",
        "# NOTES\n",
        "# make everything lowercase\n",
        "# find median length of entry = divide into small entries and large entries\n",
        "    # PROBLEM: median of drinks = 4 --> but this feels a little large\n",
        "    # allows for entries like 'Green tea or water' to count as a single label\n",
        "\n",
        "# small entries (like 2 words) will be part of data set\n",
        "# large entries -- search through for first appearing label from small entries data set\n",
        "\n",
        "####\n",
        "# bag of words to make own dataset\n",
        "# yap sessions -- take the label that appears first (this is likely their instinct)\n",
        "# if negated words (no, not, dont) = 'None'\n",
        "  # maybe just all the hard to classify ones\n",
        "  # put all the hard to classify words into separate set -- analyze later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dngAIDvCphs"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PXHKdnNCt5x"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('\\content\\test.csv')\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxGzbSCSDNtN"
      },
      "outputs": [],
      "source": [
        "#### isolate the column with the data relating to drinks (Q6)\n",
        "q6 = df[['Q6: What drink would you pair with this food item?']] # q6 is a dataframe\n",
        "q6 = q6.rename(columns={'Q6: What drink would you pair with this food item?': 'drinks'})  # making it easier to reference this column\n",
        "q6['drinks'] = q6['drinks'].str.lower() # q6 is one column = Pandas Series, also lowercasing all entries\n",
        "# print(q6['drinks'])\n",
        "\n",
        "#### divide based on median, so we divide the yapping rants vs actual titles (assuming its likely chances)\n",
        "word_lengths = q6['drinks'].str.split().explode().str.len()   # this is a series (same order as q6['drinks']) of num words in each entry\n",
        "print(word_lengths)\n",
        "# Compute the median word length across the entire column\n",
        "median_word_length = word_lengths.median()  # this is a float\n",
        "print('median = ', median_word_length)\n",
        "######### PROBLEMS\n",
        "\n",
        "# med_len(q6, 'drinks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NnuSQ1uD067"
      },
      "outputs": [],
      "source": [
        "#### define a function to isolate a set of entries that are greater than 2\n",
        "def divide_into_short_long(df, col):\n",
        "  df = df.copy()\n",
        "  df['num_words'] = df[col].str.split().str.len()  # counting number words in each entry\n",
        "  df['original_index'] = df.index  # Store the original index\n",
        "\n",
        "  small_entries_df = df[df['num_words'] <= 2].drop(columns=['num_words'])\n",
        "  large_entries_df = df[df['num_words'] > 2].drop(columns=['num_words'])\n",
        "  return small_entries_df, large_entries_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UC-lPXE4MxQ"
      },
      "outputs": [],
      "source": [
        "#### isolate short words and long words\n",
        "import statistics\n",
        "\n",
        "small_entries, large_entries = divide_into_short_long(q6, 'drinks')\n",
        "\n",
        "#### for testing\n",
        "# print(\"small entries= \\n\")\n",
        "# print(small_entries)\n",
        "# print(\"\\n\")\n",
        "print(\"large entries= \\n\")\n",
        "large_entries.head()\n",
        "# print(statistics.median([len(entry) for entry in small_entries]))   # high key idk why I needed this function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTuwGnMKP2Cw"
      },
      "outputs": [],
      "source": [
        "# !pip install thefuzz\n",
        "# from thefuzz import fuzz, process\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-gdMpD6pkwY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Removes non-alphanumeric characters and normalizes spaces.\"\"\"\n",
        "    cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()  # Normalize spaces (replace multiple spaces with a single space)\n",
        "    return cleaned\n",
        "\n",
        "def jaro_similarity(s1, s2):\n",
        "    \"\"\"Computes the Jaro similarity between two strings.\"\"\"\n",
        "    len_s1, len_s2 = len(s1), len(s2)\n",
        "\n",
        "    if len_s1 == 0 or len_s2 == 0:\n",
        "        return 0.0\n",
        "\n",
        "    match_distance = (max(len_s1, len_s2) // 2) - 1\n",
        "    s1_matches = [False] * len_s1\n",
        "    s2_matches = [False] * len_s2\n",
        "\n",
        "    matches = 0\n",
        "    transpositions = 0\n",
        "\n",
        "    # Count matches\n",
        "    for i in range(len_s1):\n",
        "        start = max(0, i - match_distance)\n",
        "        end = min(i + match_distance + 1, len_s2)\n",
        "\n",
        "        for j in range(start, end):\n",
        "            if s2_matches[j]:  # Already matched\n",
        "                continue\n",
        "            if s1[i] != s2[j]:\n",
        "                continue\n",
        "            s1_matches[i] = True\n",
        "            s2_matches[j] = True\n",
        "            matches += 1\n",
        "            break\n",
        "\n",
        "    if matches == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Count transpositions\n",
        "    s1_mapped = [s1[i] for i in range(len_s1) if s1_matches[i]]\n",
        "    s2_mapped = [s2[i] for i in range(len_s2) if s2_matches[i]]\n",
        "\n",
        "    for i in range(len(s1_mapped)):\n",
        "        if s1_mapped[i] != s2_mapped[i]:\n",
        "            transpositions += 1\n",
        "\n",
        "    transpositions //= 2\n",
        "\n",
        "    return (matches / len_s1 + matches / len_s2 + (matches - transpositions) / matches) / 3\n",
        "\n",
        "\n",
        "def jaro_winkler_similarity(s1, s2, p=0.1):\n",
        "    \"\"\"Computes Jaro-Winkler similarity between two strings.\"\"\"\n",
        "    jaro_sim = jaro_similarity(s1, s2)\n",
        "\n",
        "    # Find the common prefix length (up to 4 characters)\n",
        "    prefix_length = 0\n",
        "    for i in range(min(len(s1), len(s2), 4)):\n",
        "        if s1[i] == s2[i]:\n",
        "            prefix_length += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return jaro_sim + (prefix_length * p * (1 - jaro_sim))\n",
        "\n",
        "\n",
        "def compute_similarity_matrix(responses, similarity_func, threshold):\n",
        "    \"\"\"Computes a similarity matrix based on the given similarity function.\"\"\"\n",
        "    # preparing to use jaro winkler similarity test\n",
        "    n = len(responses)\n",
        "    similarity_matrix = np.zeros((n, n))\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            similarity = similarity_func(responses[i], responses[j])\n",
        "            if similarity >= threshold:\n",
        "                similarity_matrix[i, j] = 1\n",
        "                similarity_matrix[j, i] = 1\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "def simple_fuzzy_match(s1, s2):\n",
        "    \"\"\"A simple fuzzy matching function similar to `fuzz.partial_ratio`.\"\"\"\n",
        "    s1, s2 = clean_text(s1).lower(), clean_text(s2).lower()\n",
        "    if s1 in s2 or s2 in s1:\n",
        "        return 100  # Exact substring match\n",
        "\n",
        "    common_chars = sum((c in s2) for c in s1)\n",
        "    return int((2 * common_chars / (len(s1) + len(s2))) * 100)\n",
        "\n",
        "def extract_best_match(query, choices, threshold=85):\n",
        "    \"\"\"Finds the best match for a query string within a list of choices.\"\"\"\n",
        "    best_match, best_score = None, 0\n",
        "    for choice in choices:\n",
        "        score = simple_fuzzy_match(query, choice)\n",
        "        if score > best_score:\n",
        "            best_match, best_score = choice, score\n",
        "    return (best_match, best_score) if best_score >= threshold else (None, 0)\n",
        "\n",
        "# Example data: list of drink responses\n",
        "responses = small_entries[['drinks', 'original_index']].copy()\n",
        "responses['drinks'] = responses['drinks'].astype(str).apply(clean_text) # Apply the cleaning function to the entire column\n",
        "response_list = responses['drinks'].to_numpy()  # converting to numpy array (bc that's what clean_text takes)\n",
        "\n",
        "# see how similar the entries are using Jaro-Winkler similarity\n",
        "threshold = 0.90  # even if do 0.8 it puts miso soup and sake together -- this is why we apply fuzzy matching afterwards\n",
        "similarity_matrix = compute_similarity_matrix(response_list, jaro_winkler_similarity, threshold)\n",
        "\n",
        "# Identify clusters\n",
        "n_components, labels = connected_components(csgraph=similarity_matrix, directed=False)\n",
        "\n",
        "# Group words & indices into clusters\n",
        "clusters = {}   # dictionary mapping the group number to the group\n",
        "for i, label in enumerate(labels):\n",
        "    clusters.setdefault(label, []).append((response_list[i], responses['original_index'].iloc[i]))\n",
        "\n",
        "## new stuff for bettering matches -- applying fuzzy match across clusters\n",
        "refined_clusters = {}\n",
        "for cluster_id, group in clusters.items():\n",
        "    for word, index in group:\n",
        "        if refined_clusters:\n",
        "            match, score = extract_best_match(word, list(refined_clusters.keys()))\n",
        "            if match:\n",
        "                refined_clusters[match].append((word, index))\n",
        "            else:\n",
        "                refined_clusters[word] = [(word, index)]\n",
        "        else:\n",
        "            refined_clusters[word] = [(word, index)]\n",
        "\n",
        "# Convert refined clusters to DataFrame\n",
        "cluster_data = []\n",
        "for cluster_id, words_indices in refined_clusters.items():\n",
        "    words = [word for word, idx in words_indices]\n",
        "    indices = [idx for word, idx in words_indices]\n",
        "    count = len(words)\n",
        "    cluster_data.append([cluster_id, count, \", \".join(words), indices])\n",
        "\n",
        "cluster_df = pd.DataFrame(cluster_data, columns=['Cluster Id', 'Count', 'Cluster Words', 'Indices'])\n",
        "cluster_df = cluster_df.sort_values(by='Count', ascending=False).reset_index(drop=True)   #  use sort_values to reorder the clusters by count\n",
        "cluster_df[\"Indices\"] = cluster_df[\"Indices\"].apply(lambda x: [int(i) for i in x])    # convert np.uint64 to int -- otherwise is weird in csv file\n",
        "\n",
        "cluster_df.to_csv('/content/clusters1.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88cCMOvZ62K8"
      },
      "outputs": [],
      "source": [
        "#### create a list of tuples: a label and its frequency\n",
        "label_freqs = []\n",
        "for cluster_id, group in refined_clusters.items():\n",
        "  # print(group, \"\\n\\n\")  # for testing\n",
        "  freq = len(group)\n",
        "  if len(label_freqs) == 0:\n",
        "    label_freqs.append((cluster_id, freq))\n",
        "  else:\n",
        "    # compare to see where (based on frequency) this label lands in the list\n",
        "    inserted = False\n",
        "    for i in range(len(label_freqs)):\n",
        "      if freq > label_freqs[i][1]:\n",
        "        label_freqs.insert(i, (cluster_id, freq))\n",
        "        inserted = True\n",
        "        break\n",
        "    if not inserted:\n",
        "      label_freqs.append((cluster_id, freq))\n",
        "print(label_freqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KKGIMCqM25v"
      },
      "outputs": [],
      "source": [
        "#### go through the large_entries and pick out which of the labels appears first\n",
        "# once you figure out which label appears first, replace the entry with that label\n",
        "\n",
        "labels = [pair[0] for pair in label_freqs]\n",
        "print(\"before\\n\")\n",
        "print(large_entries)  # to see the before and after, you have to re-run everything (otherwise this is referencing the new one)\n",
        "\n",
        "for index, row in large_entries.iterrows():\n",
        "    entry = clean_text(row['drinks'])\n",
        "    words = entry.split()\n",
        "    # iterate through labels, finds the first one that appears in words\n",
        "    matched = False\n",
        "    for label in labels:\n",
        "        if label in words:\n",
        "            # easily found a match in existing labels\n",
        "            large_entries.loc[index, 'drinks'] = label  # loc is from Pandas and allows us to directly modify specific rows/cols in df\n",
        "            matched = True\n",
        "            break\n",
        "\n",
        "    if (not matched):\n",
        "        # none of the labels appeared in this entry -- manually assign to \"none\"\n",
        "        large_entries.loc[index, 'drinks'] = 'none'\n",
        "\n",
        "\n",
        "large_entries.to_csv('/content/large_entries.csv', index=False) # Save to CSV\n",
        "print(\"after\\n\")\n",
        "print(large_entries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_VkEQGU_FQG"
      },
      "outputs": [],
      "source": [
        "#### add large_entries data into the cluster dataframe\n",
        "for i, row in large_entries.iterrows():\n",
        "    # extract data from this row\n",
        "    drink_label = row['drinks']\n",
        "    index = row['original_index']\n",
        "\n",
        "    # iterate through refined_clusters to find which one has the matching label\n",
        "    for cluster_id, words_indices in refined_clusters.items():\n",
        "        cluster_words = [word for word, _ in words_indices]\n",
        "\n",
        "        if drink_label in cluster_words:\n",
        "            # this is the cluster that has this label\n",
        "            refined_clusters[cluster_id].append((drink_label, index))  # add new data\n",
        "            break\n",
        "\n",
        "# Convert updated refined_clusters to DataFrame\n",
        "cluster_data = []\n",
        "for cluster_id, words_indices in refined_clusters.items():\n",
        "    words = [word for word, idx in words_indices]\n",
        "    indices = [idx for word, idx in words_indices]\n",
        "    count = len(words)\n",
        "\n",
        "    cluster_data.append([cluster_id, count, \", \".join(words), indices])  # Include cluster_id\n",
        "\n",
        "# Create sorted DataFrame\n",
        "cluster_df = pd.DataFrame(cluster_data, columns=['Cluster ID', 'Count', 'Cluster Words', 'Indices'])\n",
        "cluster_df = cluster_df.sort_values(by='Count', ascending=False).reset_index(drop=True)  # Sort by Count\n",
        "\n",
        "# Ensure indices are saved correctly\n",
        "cluster_df[\"Indices\"] = cluster_df[\"Indices\"].apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "# Save to CSV\n",
        "cluster_df.to_csv('/content/clusters1.csv', index=False)\n",
        "\n",
        "# Print final results\n",
        "print(cluster_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ2AmAuyCE6N"
      },
      "outputs": [],
      "source": [
        "#### manually combining some clusters (ex. coke and cola) to further clean the labels\n",
        "if \"coke\" in refined_clusters and \"cola\" in refined_clusters:\n",
        "    refined_clusters[\"coke\"].extend(refined_clusters[\"cola\"])  # Merge \"cola\" into \"coke\"\n",
        "    del refined_clusters[\"cola\"]  # Remove the \"cola\" cluster\n",
        "\n",
        "if \"ice tea\" in refined_clusters and \"nestea\" in refined_clusters:\n",
        "    refined_clusters[\"ice tea\"].extend(refined_clusters[\"nestea\"])\n",
        "    del refined_clusters[\"nestea\"]\n",
        "\n",
        "if \"boba\" in refined_clusters and \"bubble tea\" in refined_clusters:\n",
        "    refined_clusters[\"boba\"].extend(refined_clusters[\"bubble tea\"])\n",
        "    del refined_clusters[\"bubble tea\"]\n",
        "\n",
        "# if we didn't find a \"sake\" cluster_id, create one now -- trying to avoid grouping all of sake into water\n",
        "if \"sake\" not in refined_clusters:\n",
        "    refined_clusters[\"sake\"] = []\n",
        "\n",
        "# find the prospective labels that have \"sake\" in them\n",
        "sake_merge_targets = [cluster_id for cluster_id in refined_clusters if \"sake\" in cluster_id and cluster_id != \"sake\"]\n",
        "\n",
        "for cluster_id in sake_merge_targets:\n",
        "    refined_clusters[\"sake\"].extend(refined_clusters[cluster_id])  # Merge \"cluster_id into \"water\"\n",
        "    del refined_clusters[cluster_id]  # Remove the cluster_id cluster\n",
        "\n",
        "# if we didn't find a \"water\" cluster_id, create one now -- we know there are lots of responses that have \"water\" but maybe not one that has a singular label\n",
        "if \"water\" not in refined_clusters:\n",
        "    refined_clusters[\"water\"] = []\n",
        "\n",
        "# find the prospective labels that have \"water\" in them\n",
        "water_merge_targets = [cluster_id for cluster_id in refined_clusters if \"water\" in cluster_id and cluster_id != \"water\"]\n",
        "\n",
        "for cluster_id in water_merge_targets:\n",
        "    refined_clusters[\"water\"].extend(refined_clusters[cluster_id])  # Merge \"cluster_id into \"water\"\n",
        "    del refined_clusters[cluster_id]  # Remove the cluster_id cluster\n",
        "\n",
        "# Convert refined clusters to DataFrame\n",
        "cluster_data = []\n",
        "for cluster_id, words_indices in refined_clusters.items():\n",
        "    words = [word for word, idx in words_indices]\n",
        "    indices = [idx for word, idx in words_indices]\n",
        "    count = len(words)\n",
        "\n",
        "    cluster_data.append([cluster_id, count, \", \".join(words), indices])\n",
        "\n",
        "cluster_df = pd.DataFrame(cluster_data, columns=['Cluster Id', 'Count', 'Cluster Words', 'Indices'])\n",
        "cluster_df = cluster_df.sort_values(by='Count', ascending=False).reset_index(drop=True) #  use sort_values to reorder the clusters by count\n",
        "cluster_df[\"Indices\"] = cluster_df[\"Indices\"].apply(lambda x: [int(i) for i in x])  # convert np.uint64 to int -- otherwise is weird in csv file\n",
        "cluster_df.to_csv('/content/clusters2.csv', index=False) # save to csv\n",
        "\n",
        "# Print results\n",
        "print(cluster_df)\n",
        "\n",
        "question = 'Q6: What drink would you pair with this food item?'\n",
        "# Iterate through each row in all_res_mapping\n",
        "for index, row in cluster_df.iterrows():\n",
        "    ids = row['Indices']  # Get the list of IDs for this group\n",
        "    label = row['Cluster Id']  # Get the label for this group\n",
        "\n",
        "    # Iterate through each ID in the list\n",
        "    for id in ids:\n",
        "        # match the actual df row index with id\n",
        "        match_index = df.index[df.index == id]\n",
        "\n",
        "        # If a match is found, replace the 'question' column with the label\n",
        "        if not match_index.empty:\n",
        "            df.loc[match_index, question] = label\n",
        "\n",
        "df = df.fillna('none')\n",
        "df.to_csv('test_v2.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # last min kar adjustemnts\n",
        "# q5='Q5: What movie do you think of when thinking of this food item?'\n",
        "# df[q5] = df[q5].apply(clean_text)\n",
        "\n",
        "# unique_values = df[q5].unique().tolist()\n",
        "\n",
        "# from pprint import pprint\n",
        "# pprint(unique_values)\n",
        "\n",
        "\n",
        "# df[q5] = df[q5].str.lower()\n",
        "# df[question] = df[question].str.lower()\n",
        "\n",
        "# forced_spiderman = ['spider man 2', 'spiderman', 'spider man', 'spiderman with tobey maguire',  'spiderman 2', 'spider man spider verse',]\n",
        "# forced_avengers = ['the avengers', 'avengers 1','avenegers',  'the avengers action movies',\n",
        "# 'the avengers postcredit scene', 'avangers','avengers age of ultron', 'avengers age of ultron',\n",
        "#  'avenger', 'avenger', 'the avengers action movies', 'marvel movies', 'endgame',  'avengers',  'avengers endgame', 'the avengers 2012',]\n",
        "# forced_teens = [ 'teenage mutant ninja turtle',   'ninja turtle','tnmt','ninja turtle',  'ninja turtles', 'teenage mutant ninja turtles', 'avengers end game',]\n",
        "# i_hate_you = [ 'none', 'idk',  'no movie',  'no movie comes to mind', 'not sure', 'none specifically', 'nothing',\n",
        "#  'i dont', 'i cant think of anything', '',  'i dont know', 'no', 'scific movies', 'no', 'none come to mind', 'no but i think japan',\n",
        "#  'i dont know', 'none in particular', 'na', 'nothing comes to mind', 'none specifically',]\n",
        "# froced_deadpool=['deadpool', 'deadpool 3', 'deadpool 2', 'deadpool vs wolverine',  'deadpool wolverine',]\n",
        "# forced_sponge = [  'spongebob', 'the spongebob movie 2004']\n",
        "# forced_poke=[ 'pokemon the first movie', 'pokemon']\n",
        "# forced_pizza = [ 'pizza', 'pizza 2012',]\n",
        "# forced_fnaf = [ 'five nights at freddies', 'five nights at freddys',]\n",
        "# forced_iron_man = [ 'iron man', 'iron man i',]\n",
        "# froced_home_alon = [ 'i think of home alone', 'home alone', 'home alone 2', 'home alone or action movies', 'homealone',]\n",
        "# f_fnf = [ 'fast and furious', 'fast and furious tokyo drift',  'tokyo drift',]\n",
        "# mario=[ 'mario movie', 'the mario movie',]\n",
        "# nemo=[ 'findig nemo', 'nemo',]\n",
        "# your_nae=[ 'your name 2016',]\n",
        "# j_W = [ 'john wick 3', 'john wick chapter 4',]\n",
        "# alessia = [ 'the italian job','italian movie', 'italian jon',] # italian job\n",
        "# mib = [ 'men in black','mib',  'man in black',]\n",
        "# misison_impossible=[ 'action movie like mission impossible',]\n",
        "\n",
        "\n",
        "# df[q5] = df[q5].replace(forced_spiderman, 'spiderman')\n",
        "# df[q5] = df[q5].replace(forced_avengers, 'the avengers')\n",
        "# df[q5] = df[q5].replace(forced_teens, 'teenage mutant ninja turtles')\n",
        "# df[q5] = df[q5].replace(i_hate_you, 'none')\n",
        "# df[q5] = df[q5].replace(froced_deadpool, 'deadpool')\n",
        "# df[q5] = df[q5].replace(forced_sponge, 'spongebob')\n",
        "# df[q5] = df[q5].replace(forced_poke, 'pokemon')\n",
        "# df[q5] = df[q5].replace(forced_pizza, 'pizza')\n",
        "# df[q5] = df[q5].replace(forced_fnaf, 'five nights at freddys')\n",
        "# df[q5] = df[q5].replace(forced_iron_man, 'iron man')\n",
        "# df[q5] = df[q5].replace(froced_home_alon, 'home alone')\n",
        "# df[q5] = df[q5].replace(f_fnf, 'fast and furious')\n",
        "# df[q5] = df[q5].replace(mario, 'mario movie')\n",
        "# df[q5] = df[q5].replace(nemo, 'finding nemo')\n",
        "# df[q5] = df[q5].replace(j_W, 'john wick')\n",
        "# df[q5] = df[q5].replace(alessia, 'the italian job')\n",
        "# df[q5] = df[q5].replace(mib, 'men in black')\n",
        "# df[q5] = df[q5].replace(misison_impossible, 'mission impossible')\n",
        "\n",
        "# ##### repeats  #######\n",
        "\n",
        "# question='Q5: What movie do you think of when thinking of this food item?'\n",
        "# q5 = df[['id', question]]\n",
        "\n",
        "# # Extract the 'id' column and the responses\n",
        "# ids = q5['id']  # Store the 'id' column\n",
        "# responses = q5[question]  # Extract only the responses\n",
        "\n",
        "# # Reset the index of the responses Series\n",
        "# responses = responses.reset_index(drop=True)  # Reset index to 0, 1, 2, ...\n",
        "\n",
        "# # Compute the similarity matrix using Jaro-Winkler similarity\n",
        "# threshold = 0.87\n",
        "# similarity_matrix = compute_similarity_matrix(\n",
        "#     responses, jellyfish.jaro_winkler_similarity, threshold\n",
        "# )\n",
        "\n",
        "# # Identify clusters using connected components\n",
        "# n_components, labels = connected_components(csgraph=similarity_matrix, directed=False)\n",
        "\n",
        "# # Group responses and their corresponding IDs based on clusters\n",
        "# clusters = {}\n",
        "# for i, label in enumerate(labels):\n",
        "#     if label not in clusters:\n",
        "#         clusters[label] = {'responses': [], 'ids': []}\n",
        "#     clusters[label]['responses'].append(responses[i])  # Store the response\n",
        "#     clusters[label]['ids'].append(ids.iloc[i])  # Store the corresponding ID\n",
        "\n",
        "\n",
        "# # Convert clusters into a DataFrame\n",
        "# cluster_data = []\n",
        "# for cluster_id, group in clusters.items():\n",
        "#     cluster_data.append({\n",
        "#         'Cluster ID': cluster_id,\n",
        "#         'Responses': ', '.join(group['responses']),  # Combine responses into a single string\n",
        "#         'IDs': ', '.join(map(str, group['ids'])),  # Combine IDs into a single string\n",
        "#         'Number of Responses': len(group['responses'])  # Add count of responses in the cluster\n",
        "#     })\n",
        "\n",
        "# # Create the DataFrame\n",
        "# clusters_df = pd.DataFrame(cluster_data)\n",
        "# clusters_df = clusters_df.sort_values(by='Number of Responses', ascending=False)\n",
        "# # Apply the function to the 'Responses' column to create the 'Label' column\n",
        "# clusters_df['Label'] = clusters_df['Responses'].apply(ensure_list).apply(find_median_length_string)\n",
        "\n",
        "\n",
        "# # Iterate through each row in all_res_mapping\n",
        "# for index, row in clusters_df.iterrows():\n",
        "#     ids = row['IDs']  # Get the list of IDs for this group\n",
        "#     label = row['Label']  # Get the label for this group\n",
        "\n",
        "#     # Iterate through each ID in the list\n",
        "#     for id in ids:\n",
        "#         # Find the row in qq where the 'id' column matches the current ID\n",
        "#         match_index = df[df['id'] == id].index\n",
        "\n",
        "#         # If a match is found, replace the 'question' column with the label\n",
        "#         if not match_index.empty:\n",
        "#             df.loc[match_index, question] = label\n",
        "\n",
        "# df = df.fillna('none')\n",
        "\n",
        "# df.to_csv('/content/clusters3.csv', index=False) # save to csv\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uFS1w5QtUEqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7vmHPqYcexy"
      },
      "outputs": [],
      "source": [
        "# # Iterate through each row in all_res_mapping\n",
        "# for index, row in all_res_mapping.iterrows():\n",
        "#     ids = row['IDs']  # Get the list of IDs for this group\n",
        "#     label = row['Label']  # Get the label for this group\n",
        "\n",
        "#     # Iterate through each ID in the list\n",
        "#     for id in ids:\n",
        "#         # Find the row in qq where the 'id' column matches the current ID\n",
        "#         match_index = model_df[model_df['id'] == id].index\n",
        "\n",
        "#         # If a match is found, replace the 'question' column with the label\n",
        "#         if not match_index.empty:\n",
        "#             model_df.loc[match_index, question] = label\n",
        "\n",
        "# model_df = model_df.fillna('None')\n",
        "\n",
        "\n",
        "# # Save the updated qq DataFrame to a CSV file (optional)\n",
        "# model_df.to_csv('/content/updated_qq.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LcVxLjPOmtM"
      },
      "outputs": [],
      "source": [
        "# !pip install pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import jellyfish\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove non-alphanumeric characters except spaces\n",
        "    cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Normalize spaces (replace multiple spaces with a single space)\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "    return cleaned\n",
        "\n",
        "model_df = pd.read_csv('test_v2.csv')\n",
        "question = 'Q5: What movie do you think of when thinking of this food item?'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# filtering manually\n",
        "forced_spiderman = ['into the spiderverse', 'spiderman just cause it takes place in new york', 'i think of spiderman when thinking of this food item','spider man 2','sam raimis spider man',  'i think of the spiderman movie', 'spiderman', 'spider man', 'spiderman with tobey maguire',  'spiderman 2', 'spiderman 2002 starring tobey maguire']\n",
        "forced_avengers = ['the avengers', 'i think of the avengers when i think of shawarma',\n",
        "'i think of the avengers when thinking of this food item', 'i dont usually think of any movie when having this food item but after using google to refresh my memory i would say avengers because of the postcredits scene','i think of the movie the avengers because in the final scene of the movie tony stark takes the avengers to the shawarma place',\n",
        "'i think about the movie avengers when i think of shawarma','i think of the movie named avengers', 'i think of the movie the avengers','i think of the movie the avengers they have an entire bit surrounding this food item', 'avengers end game', 'avengers 1','avenegers',  'the avengers action movies',\n",
        "'the avengers postcredit scene', 'avangers','avengers age of ultron', 'avengers age of ultron','marvel avengers', 'marvels avengers','advengers 2012 movie', 'avengers 2012',\n",
        " 'avenger', 'avenger', 'that one marvel movie i dont know i dont watch movies','not exactly movies but i would prefer to watch some tv series when i am having pizzas if movies specifically i may think of cartoon movies or more heromarvel related ones like teenage mutant ninja turtles movies in marvel series etc',\n",
        "                   'the avengers because they eat it together after the battle of new york',\n",
        "                   'the avengers movie when they eat shawarma in the end credits scene','avengers because of iron man asking everyone to get shawarma after the fight',\n",
        "                   'the only movie that is associated with this food item would be the first avengers due to the post credit scene',\n",
        "'the avengers at the end of the film after the battle in nyc tony stark suggests to the rest of the team that they go get shawarma'\t,\n",
        "                   'the avengers action movies', 'marvel movies', 'endgame',  'avengers',  'avengers endgame', 'the avengers 2012']\n",
        "forced_teens = [ 'teenage mutant ninja turtle', 'rise of the teenage mutant ninja turtles the movie 2022',\n",
        "                'teenage mutant ninja turtles vs batman the tmnt love pizza and they eat it with batman at the end of the movie',\n",
        "                'i think of the ninja turtle movies when thinking of pizza', 'the teenage mutant ninja turtles', 'ninja turtle','tnmt','ninja turtle',  'ninja turtles', 'teenage mutant ninja turtles',  'any teenage mutant ninja turtle movie', 'inside out teenage mutant ninja turtles']\n",
        "i_hate_you = [ 'none', 'none ok tbf i think of my life at uoft if life is a movie','no movies come to mind', 'no movie came up to my mind','idk', 'i actually can not think of anything', 'no movie', 'no movie comes to mind', 'not sure', 'none specifically', 'nothing',\n",
        " 'i dont', 'i cant think of anything', '', 'no movies come to mind',  'i dont know', 'no', 'scific movies', 'no', 'none come to mind', 'no but i think japan', 'i dont think of any movie', 'i dont think of any movie', 'i dont think of a movie', 'i dont watch movies',\n",
        " 'i dont know', 'none in particular', 'na', 'na i only watched a few movies so i cannot think of any movie related to pizza',\n",
        "'na i only watched a few movies so i cannot think of any movie related to shawarma', 'i dont have a movie i think of when i think of pizza',\n",
        "               'ive never had them before attending u of t so i dont associate them with anything',\n",
        "'na i only watched a few movies so i cannot think of any movie related to sushi',\t'none stranger things is the tv series i can think of',\n",
        "'na i dont watch a lot of movies', 'i dont know that many movies fr','i have no idea i dont eat that stuff',\n",
        "'na i dont watch a lot of movies', 'i think about the first avengers movie after they fought off the aliens in the city the superheroes gather at a local shawarma place that is nearly destroyed because of the alien invasion and eat quietly around the table',\n",
        "'na i dont watch a lot of movies', 'i dont have a movie i think of when i think of sushi','i dont think of a movie when thinking of this food item','i dont think of any movie pizza pairs well with any movie',\n",
        "               'i might think of some short ones or simply videos as when i am having shawarma i do not usually have much time and the food is some hands size so people can usually finish it up quickly so i would say no movieshort ones like monster inc',\n",
        "               'probably some niche movie or some movie specific to a country and not an english movie',\n",
        "               'nothing comes to mind', 'no movie that i can think','none specifically']\n",
        "froced_deadpool=['deadpool', 'deadpool 3', 'deadpool 2','deadpool wolverine is the movie that comes to mind this is because of the ending scene where they get this food after their battle',\n",
        "\n",
        "                 'deadpool vs wolverine',  'deadpool wolverine']\n",
        "forced_sponge = [  'spongebob', 'the spongebob movie 2004']\n",
        "forced_poke=[ 'pokemon the first movie', 'pokemon']\n",
        "forced_pizza = [ 'pizza', 'pizza 2012', 'honestly any movie pizza is a movie food', 'i think about a nice chill movie with pizza maybe something like hangover']\n",
        "forced_fnaf = [ 'five nights at freddies', 'five nights at freddys']\n",
        "forced_iron_man = [ 'iron man', 'iron man i']\n",
        "froced_home_alon = [ 'i think of home alone', 'for some reason home alone is the first thing that came to mind',\n",
        "                    'i usually associate this item with the winter because school parties would frequently order this food item hence i would say the polar express elf and home alone',\n",
        "                    'home alone because kevin orders an entire one for himself','home alone when the delivery guy delivers a cheese pizza little neros pizza through the back door for kevin when he gets stuck home alone',\n",
        "                     \"\"\"home alone is the first movie that comes to minds im referring to the scene were the whole family order 10 boxes of pizza and their trying to see who is going to pay\"\"\",'i think about about the movie home alone','home alone', 'home alone 2 lost in new york', 'home alone 2 lost in new york','home alone 2', 'home alone or action movies', 'homealone']\n",
        "f_fnf = [ 'fast and furious', 'fast and furious tokyo drift',  'tokyo drift']\n",
        "mario=[ 'mario movie', 'the mario movie']\n",
        "nemo=[ 'findig nemo', 'nemo', 'finding nemo was the first movie in my mind']\n",
        "your_nae=[ 'your name 2016']\n",
        "j_W = [ 'john wick 3', 'john wick chapter 4', 'john wick chapter 3 parabellum', 'john wick chapter 3 parabellum']\n",
        "alessia = [ 'the italian job','italian movie', 'italian jon', 'i think of the movie little italy the entire plot revolves around two individuals who are competing in a restaurant business where their primary product is this food item'] # italian job\n",
        "mib = [ 'men in black','mib',  'man in black']\n",
        "misison_impossible=[ 'action movie like mission impossible']\n",
        "borat=['i think of the movie borat']\n",
        "wolv = ['the wolverine', 'the wolverine', 'the wolverine', 'the wolverine','wolverine']\n",
        "\n",
        "jap = ['any japanese movie and animes', 'tokyo story 1953', 'tokyo story', 'a japanese anime movie such as spirited away',\n",
        "       'some japanese movie or some action movie', 'spirited away or any japanese movie in general',\n",
        "       'i think of ninja movies with i think of sushi especially japanese anime ninja movies','i dont think of any movies i think of anime',\n",
        "       'sushi is from japan so i would think about a japanese movie your name the anime movie', 'almost all asian movies especially japanese not any movie in particular comes in mind'\n",
        "\n",
        "       ]\n",
        "spiderverse=['into the spiderverse', ]\n",
        "totoro=['some ghibli movie', ]\n",
        "lionking=['mufasa the lion king']\n",
        "harry=['harry potter the philosophers stone', 'i watched the harry potter series with my friends while eating sushi so i associate sushi with harry potter']\n",
        "bruh = ['something fun like superbad']\n",
        "sushi=['dreams of sushi', 'i might came up with some japan related movieslike jiro dreams of sushi this is in fact a documentarycartoonslike tv series like the solitary gourmet',\n",
        "       'i think of the movie jiro dreams of sushi', 'i think of the movie jiro dreams of sushi it is a movie about an 85 year old sushi master behind a three michelin star sushi restaurant and his sons who are set to take over the business']\n",
        "pasgetti = ['spaghetti with a chance of meatballs', 'i think of the movie cloudy with a chance of meatballs', 'i think of the movie cloudy with a chance of meatballs because i associate pizza with popular italian cuisine and meatballs is italian cuisine']\n",
        "garfield=['the garfield movie', 'the garfield movie']\n",
        "\n",
        "\n",
        "print(model_df[question].value_counts())\n",
        "\n",
        "model_df[question] = model_df[question].astype(str)\n",
        "model_df[question] = model_df[question].str.lower()\n",
        "model_df[question] = model_df[question].apply(lambda x: clean_text(str(x)) if pd.notnull(x) else '')\n",
        "\n",
        "model_df[question] = model_df[question].replace(forced_spiderman, 'spiderman')\n",
        "model_df[question] = model_df[question].replace(forced_avengers, 'the avengers')\n",
        "model_df[question] = model_df[question].replace(forced_teens, 'teenage mutant ninja turtles')\n",
        "model_df[question] = model_df[question].replace(i_hate_you, 'none')\n",
        "model_df[question] = model_df[question].replace(froced_deadpool, 'deadpool')\n",
        "model_df[question] = model_df[question].replace(forced_sponge, 'spongebob')\n",
        "model_df[question] = model_df[question].replace(forced_poke, 'pokemon')\n",
        "model_df[question] = model_df[question].replace(forced_pizza, 'pizza')\n",
        "model_df[question] = model_df[question].replace(forced_fnaf, 'five nights at freddys')\n",
        "model_df[question] = model_df[question].replace(forced_iron_man, 'iron man')\n",
        "model_df[question] = model_df[question].replace(froced_home_alon, 'home alone')\n",
        "model_df[question] = model_df[question].replace(f_fnf, 'fast and furious')\n",
        "model_df[question] = model_df[question].replace(mario, 'mario movie')\n",
        "model_df[question] = model_df[question].replace(nemo, 'finding nemo')\n",
        "model_df[question] = model_df[question].replace(j_W, 'john wick')\n",
        "model_df[question] = model_df[question].replace(alessia, 'the italian job')\n",
        "model_df[question] = model_df[question].replace(mib, 'men in black')\n",
        "model_df[question] = model_df[question].replace(misison_impossible, 'mission impossible')\n",
        "model_df.loc[:, question] = model_df[question].replace('i think of the movie borat', 'borat')\n",
        "model_df[question] = model_df[question].replace(wolv, 'wolverine')\n",
        "model_df[question] = model_df[question].replace(jap, 'japanese')\n",
        "model_df[question] = model_df[question].replace(totoro, 'totoro')\n",
        "model_df[question] = model_df[question].replace(lionking, 'lion king')\n",
        "model_df[question] = model_df[question].replace(harry, 'harry potter')\n",
        "model_df[question] = model_df[question].replace(bruh, 'superbad')\n",
        "model_df[question] = model_df[question].replace(sushi, 'jiro dreams of sushi')\n",
        "model_df[question] = model_df[question].replace(pasgetti, 'cloudy with a chance of meatball')\n",
        "model_df[question] = model_df[question].replace(garfield, 'garfield')\n",
        "\n",
        "\n",
        "\n",
        "ratatouil=[\n",
        "    \"\"\" it sounds like youre referring to the movie ratatouille its a pixar animated film about a rat named remy who dreams of becoming a great chef despite being a rodent in a profession dominated by humans the story is set in paris and it beautifully blends themes of passion creativity and perseverance in the culinary world let me know if this is the one youre thinking of\"\"\",\n",
        "    'rattatouie even though there isnt pizza','ratatouille again but thats because my knowledge of movies is limited and i barely watch any',\n",
        "           'ratatouille because the rat is italian and italians like pizza', 'i think about the movie ratatouille as it took place in italy and pizza reminds me of italy',\n",
        "            'ratatouille because sushi needs to be sliced and they slice food in ratatouille',\n",
        "            'ratatouille because the rat cooks food and this is food','it sounds like youre referring to the movie ratatouille its a pixar animated film about a rat named remy who dreams of becoming a great chef despite being a rodent in a profession dominated by humans the story is set in paris and it beautifully blends themes of passion creativity and perseverance in the culinary world let me know if this is the one youre thinking of, it sounds like youre referring to the movie ratatouille its a pixar animated film about a rat named remy who dreams of becoming a great chef despite being a rodent in a profession dominated by humans the story is set in paris and it beautifully blends themes of passion creativity and perseverance in the culinary world let me know if this is the one youre thinking of, it sounds like youre referring to the movie ratatouille its a pixar animated film about a rat named remy who dreams of becoming a great chef despite being a rodent in a profession dominated by humans the story is set in paris and it beautifully blends themes of passion creativity and perseverance in the culinary world let me know if this is the one youre thinking of']\n",
        "ur_name=['i think of the movie your name', 'i think of your name when thinking of this food item',]\n",
        "mulan=['think of mulan which my family and i watched last weekend while eating shawarma']\n",
        "\n",
        "bigbigwolf=['i would associate it with movie series like pleasant goat and big big wolf as that was one of the childhood shows i watched from the time when we could sometimes go and eat out as a family']\n",
        "thebigshort=['the movie that comes to mind for this food item is the big short there is one scene that is very vivid and eyeopening conversations when it comes to the fault of hedge funds during the 2008 crisis that was discussed over this food item']\n",
        "starwqrs=['star wars episode v the empire strikes back']\n",
        "scottpilgtam=['i think of movies that take place in toronto since its so common here such as scott pilgrim vs the world']\n",
        "\n",
        "#\tmiddle east movie\n",
        "middle_east=['probably something with a middle eastern setting', 'i think of middle easternindian movies but nothing some in mind specifically',\n",
        "             'probably some middle eat movies', 'those commercials by the toronto food man za bebsi is the best']\n",
        "legendshawrama=['none only thing i know about it is the game shawarma legend', 'not suremaybe turkish movies but it will remind me of a game called shawarma legend which is a simulation game and highly recommended',\n",
        "                'cant think of any dont have an idea about the taste of shawarma and havent seen it in any movie that ive watched']\n",
        "shrekkk=['i think about movies like holes or shrek since it reminds me of the movies teachers would put on for us during school events',\n",
        "         'when eating pizza i think of the movie shrek 2', 'when eating shawarma i think of the movie shrek 2', 'when eating sushi i think of the movie shrek 2']\n",
        "chef=['i think of the movie chef2014 as it was very foodrelated and had a lot of ovenbaked goods','i think of the movie named chef',  'we live in time in one of the scenes the chef is making sushi i watched it recently']\n",
        "food_wars =['i thought of the anime food wars', 'i think of the anime food wars']\n",
        "jaws=['i think of the movie jaws because sushi is fish and jaws is a movie about shark which is a fish']\n",
        "cheech_chong=['i think of the cheech chong movies when i think of shawarma']\n",
        "goodtime=['i think about the movie good time when thinking of this food item']\n",
        "isledogs = ['isle of dogs this film showcases a chef making a poisoned sushi']\n",
        "brkfast=['breakfast club one of the characters gets into an argument with someone else if eating sushi is allowed']\n",
        "brkabad=['i actually think of the series breaking bad and that famous scene where walter flips out']\n",
        "docs = ['bbc food channels movies and documentaries']\n",
        "\n",
        "penguinismagdas=['penguins of madagascar there was a scene where all penguins made sushi and had to fight a big purple octopus with it']\n",
        "cars=['i think of cars 2 when thinking of this food item this is because of the scene where tow mater mistakes wasabi a popular topping for this food item for pistachio ice cream with comical results']\n",
        "toystory=['toy story the restaurant that the green aliens come from serve it the name of the restaurant is an alliteration with the last word of the name being planet']\n",
        "malena=['when thinking of pizza it would reminder me of italy and the first movie that came to mind was malena']\n",
        "minions=['to be honest i cant really think of any movie but if i have to answer i would say minions rise of gru']\n",
        "gilmore_girls=['i think of the show gilmore girls theyre always eating pizza']\n",
        "goodfellas=['goodfellas there is a restaurant that sells pizza and is goodfellas themed']\n",
        "cartoons=['im thinking of cartoons and mostly western based cartoon though i cant think of anything specific but mostly western cartoon movies']\n",
        "godfather=['i think of italian mobster movies like the godfather']\n",
        "\n",
        "bruhh=['it sounds like youre referring to the movie ratatouille its a pixar animated film about a rat named remy who dreams of becoming a great chef despite being a rodent in a profession dominated by humans the story is set in paris and it beautifully blends themes of passion creativity and perseverance in the culinary world let me know if this is the one youre thinking of']\n",
        "\n",
        "model_df[question] = model_df[question].replace(bruhh, 'ratatouille')\n",
        "model_df[question] = model_df[question].replace(ratatouil, 'ratatouille')\n",
        "model_df[question] = model_df[question].replace(ur_name, 'your name')\n",
        "model_df[question] = model_df[question].replace(mulan, 'mulan')\n",
        "model_df[question] = model_df[question].replace(bigbigwolf, 'big big wolf')\n",
        "model_df[question] = model_df[question].replace(thebigshort, 'the big short')\n",
        "model_df[question] = model_df[question].replace(starwqrs, 'star wars')\n",
        "model_df[question] = model_df[question].replace(scottpilgtam, 'scott pilgrim vs the world')\n",
        "model_df[question] = model_df[question].replace(middle_east, 'middle east movie')\n",
        "model_df[question] = model_df[question].replace(legendshawrama, 'shawarma legend')\n",
        "model_df[question] = model_df[question].replace(shrekkk, 'shrek')\n",
        "model_df[question] = model_df[question].replace(chef, 'chef')\n",
        "model_df[question] = model_df[question].replace(food_wars, 'food wars')\n",
        "model_df[question] = model_df[question].replace(jaws, 'jaws')\n",
        "model_df[question] = model_df[question].replace(cheech_chong, 'cheech chong')\n",
        "model_df[question] = model_df[question].replace(goodtime, 'good time')\n",
        "model_df[question] = model_df[question].replace(isledogs, 'isle of dogs')\n",
        "model_df[question] = model_df[question].replace(brkfast, 'breakfast club')\n",
        "model_df[question] = model_df[question].replace(brkabad, 'breaking bad')\n",
        "model_df[question] = model_df[question].replace(docs, 'documentary')\n",
        "model_df[question] = model_df[question].replace(penguinismagdas, 'pengnuins of madagascar')\n",
        "model_df[question] = model_df[question].replace(cars, 'cars')\n",
        "model_df[question] = model_df[question].replace(toystory, 'toy story')\n",
        "model_df[question] = model_df[question].replace(malena, 'malena')\n",
        "model_df[question] = model_df[question].replace(minions, 'minions')\n",
        "model_df[question] = model_df[question].replace(gilmore_girls, 'gilmore girls')\n",
        "model_df[question] = model_df[question].replace(goodfellas, 'goodfellas')\n",
        "model_df[question] = model_df[question].replace(cartoons, 'cartoons')\n",
        "model_df[question] = model_df[question].replace(godfather, 'the godfather')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(model_df[question].value_counts())\n",
        "\n",
        "q5 = model_df[['id', question]]\n",
        "\n",
        "q5[question] = q5[question].str.lower()\n",
        "q5[question] = q5[question].astype(str)\n",
        "q5['word_count'] = q5[question].str.split().str.len()\n",
        "q5 = q5.sort_values(by='word_count', ascending=False)  # Use ascending=True for increasing order\n",
        "\n",
        "\n",
        "# divide based on median, so we divide the yapping rants vs actual titles (assuming its likely chances)\n",
        "# actually, not very satisfied with this median length....\n",
        "word_lengths = q5[question].str.split().explode().str.len()\n",
        "median_word_length = word_lengths.quantile(0.5)\n",
        "print(median_word_length)\n",
        "\n",
        "# ... we did more exploration\n",
        "# Compute the median word length across the entire column\n",
        "median_word_length = word_lengths.quantile(0.95)\n",
        "print(median_word_length)\n",
        "\n",
        "# Count the occurrences of each word length\n",
        "word_length_counts = word_lengths.value_counts().sort_index()\n",
        "\n",
        "# Plot the counts for each word length\n",
        "plt.figure(figsize=(14, 6))  # Increase figure width to provide more space\n",
        "bars = plt.bar(word_length_counts.index, word_length_counts.values, color='skyblue')\n",
        "plt.title(\"Distribution of Word Lengths\", fontsize=16)\n",
        "plt.xlabel(\"Word Length\", fontsize=14)\n",
        "plt.ylabel(\"Count\", fontsize=14)\n",
        "plt.xticks(range(1, 35), rotation=45, ha='right', fontsize=10)  # Rotate labels and align them to the right\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()  # Adjust layout to prevent label overlap\n",
        "plt.show()\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "# analyzing this shows us that from 6-7 words is split between real movie\n",
        "# names and long answers. Will use the cut-off of 6<= words for the first part\n",
        "# of processing and >6 that has additional processing\n",
        "q5.to_csv('/content/word_counted_responses.csv', index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NUq7ONm6uUR"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\n",
        "# Now let's start classifying based on the data set:\n",
        "# \"\"\"\n",
        "\n",
        "# Extract the 'id' column and the responses\n",
        "ids = q5[q5['word_count'] < 7]['id']  # Store the 'id' column\n",
        "responses = q5[q5['word_count'] < 7][question]  # Extract only the responses\n",
        "\n",
        "# Reset the index of the responses Series\n",
        "responses = responses.reset_index(drop=True)  # Reset index to 0, 1, 2, ...\n",
        "responses = responses.apply(clean_text)\n",
        "\n",
        "\n",
        "# Function to compute the similarity matrix\n",
        "def compute_similarity_matrix(responses, similarity_func, threshold):\n",
        "    n = len(responses)\n",
        "    similarity_matrix = np.zeros((n, n))\n",
        "\n",
        "    # Vectorized computation of pairwise similarities\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):  # Only compute upper triangle\n",
        "            similarity = similarity_func(responses[i], responses[j])\n",
        "            if similarity >= threshold:\n",
        "                similarity_matrix[i, j] = 1\n",
        "                similarity_matrix[j, i] = 1  # Symmetric\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "# Compute the similarity matrix using Jaro-Winkler similarity\n",
        "threshold = 0.87\n",
        "similarity_matrix = compute_similarity_matrix(\n",
        "    responses, jaro_winkler_similarity, threshold\n",
        ")\n",
        "\n",
        "# Identify clusters using connected components\n",
        "n_components, labels = connected_components(csgraph=similarity_matrix, directed=False)\n",
        "\n",
        "# Group responses and their corresponding IDs based on clusters\n",
        "clusters = {}\n",
        "for i, label in enumerate(labels):\n",
        "    if label not in clusters:\n",
        "        clusters[label] = {'responses': [], 'ids': []}\n",
        "    clusters[label]['responses'].append(responses[i])  # Store the response\n",
        "    clusters[label]['ids'].append(ids.iloc[i])  # Store the corresponding ID\n",
        "\n",
        "############## for analysis ################\n",
        "\n",
        "# Output clusters\n",
        "for cluster_id, group in clusters.items():\n",
        "    print(f\"Group {cluster_id}: Responses: {group['responses']}, IDs: {group['ids']}\")\n",
        "\n",
        "# Convert clusters into a DataFrame\n",
        "cluster_data = []\n",
        "for cluster_id, group in clusters.items():\n",
        "    cluster_data.append({\n",
        "        'Cluster ID': cluster_id,\n",
        "        'Responses': ', '.join(group['responses']),  # Combine responses into a single string\n",
        "        'IDs': ', '.join(map(str, group['ids'])),  # Combine IDs into a single string\n",
        "        'Number of Responses': len(group['responses'])  # Add count of responses in the cluster\n",
        "    })\n",
        "\n",
        "# Create the DataFrame\n",
        "clusters_df = pd.DataFrame(cluster_data)\n",
        "clusters_df = clusters_df.sort_values(by='Number of Responses', ascending=False)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "clusters_df.to_csv('workin_smaller_class.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Output the DataFrame\n",
        "print(clusters_df)\n",
        "df_i = clusters_df\n",
        "\n",
        "\"\"\"\n",
        "# List of IDs to merge\n",
        "ids_to_merge = [268, 23 271  98 290  14   ]\n",
        "\n",
        "\n",
        "ids_to_merge_avengers = [66, 72, 158, ]\n",
        "turtles = [  223,            ]\n",
        "\n",
        "df_i = clusters_df\n",
        "\n",
        "# Filter rows with the specified IDs\n",
        "filtered_df = df_i[df_i['Cluster ID'].isin(ids_to_merge)]\n",
        "\n",
        "print(filtered_df)\n",
        "\n",
        "# Ensure 'Responses' and 'IDs' are lists (handle cases where they are strings)\n",
        "def ensure_list(value):\n",
        "    if isinstance(value, str):  # If it's a string, split it into a list\n",
        "        return value.split(', ')\n",
        "    return value  # Otherwise, assume it's already a list\n",
        "\n",
        "\n",
        "# Aggregate the data\n",
        "merged_row = {\n",
        "    'Cluster ID': ids_to_merge[0],  # Use the first ID as the label\n",
        "    'Responses': sum(filtered_df['Responses'].apply(ensure_list).tolist(), []),  # Concatenate lists of responses\n",
        "    'IDs': sum(filtered_df['IDs'].apply(ensure_list).tolist(), []),  # Concatenate lists of IDs\n",
        "    'Number of Responses': filtered_df['Number of Responses'].sum(),  # Sum the count\n",
        "}\n",
        "\n",
        "# Create a new DataFrame for the merged row\n",
        "merged_df = pd.DataFrame([merged_row])\n",
        "\n",
        "# Remove the merged rows from the original DataFrame\n",
        "df_i = df_i[~df_i['Cluster ID'].isin(ids_to_merge)]\n",
        "\"\"\"\n",
        "\n",
        "def ensure_list(value):\n",
        "    if isinstance(value, str):  # If it's a string, split it into a list\n",
        "        return value.split(', ')\n",
        "    return value  # Otherwise, assume it's already a list\n",
        "\n",
        "\n",
        "# Function to find the median-length string in a list\n",
        "def find_median_length_string(string_list):\n",
        "    if not string_list:  # Handle empty lists\n",
        "        return None\n",
        "    # Sort the list by string length\n",
        "    sorted_list = sorted(string_list, key=lambda x: len(x))\n",
        "    # Find the median index\n",
        "    median_index = len(sorted_list) // 2\n",
        "    return sorted_list[median_index]  # Return the entire string\n",
        "\n",
        "# Apply the function to the 'Responses' column to create the 'Label' column\n",
        "df_i['Label'] = df_i['Responses'].apply(ensure_list).apply(find_median_length_string)\n",
        "df_i.to_csv('/content/clusters_output_v2.csv', index=False)\n",
        "\n",
        "\n",
        "# Iterate through each row in all_res_mapping\n",
        "for index, row in df_i.iterrows():\n",
        "    ids = row['IDs']  # Get the list of IDs for this group\n",
        "    label = row['Label']  # Get the label for this group\n",
        "\n",
        "    # Iterate through each ID in the list\n",
        "    for id in ids:\n",
        "        # Find the row in qq where the 'id' column matches the current ID\n",
        "        match_index = model_df[model_df['id'] == id].index\n",
        "\n",
        "        # If a match is found, replace the 'question' column with the label\n",
        "        if not match_index.empty:\n",
        "            model_df.loc[match_index, question] = label\n",
        "\n",
        "model_df = model_df.fillna('none')\n",
        "\n",
        "import pandas as pd\n",
        "import copy\n",
        "import ast\n",
        "\n",
        "stopwords = [\n",
        "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\n",
        "    \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
        "    'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n",
        "    'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\",\n",
        "    'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n",
        "    'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
        "    'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
        "    'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from',\n",
        "    'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once',\n",
        "    'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
        "    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
        "    'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now',\n",
        "    'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn',\n",
        "    \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn',\n",
        "    \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\",\n",
        "    'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
        "]\n",
        "\n",
        "\n",
        "# Function to remove stopwords from a text\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()  # Split the text into words\n",
        "    filtered_words = [word for word in words if word.lower() not in stopwords]  # Remove stopwords\n",
        "    return ' '.join(filtered_words)  # Join the words back into a sentence\n",
        "\n",
        "# long_words_bombo = q5[q5['word_count'] > 6][question].to_list()\n",
        "# model_df[question] = model_df[question].apply(lambda x: remove_stopwords(x) if str(x) in long_words_bombo else x)\n",
        "\n",
        "# part 2 beware same variable names\n",
        "ids = model_df['id']  # Store the 'id' column\n",
        "responses = model_df[question]  # Extract only the responses\n",
        "responses = responses.reset_index(drop=True)  # Reset index to 0, 1, 2, ...\n",
        "\n",
        "similarity_matrix = compute_similarity_matrix(\n",
        "    responses, jaro_winkler_similarity, threshold\n",
        ")\n",
        "\n",
        "# Identify clusters using connected components\n",
        "n_components, labels = connected_components(csgraph=similarity_matrix, directed=False)\n",
        "\n",
        "# Group responses and their corresponding IDs based on clusters\n",
        "clusters = {}\n",
        "for i, label in enumerate(labels):\n",
        "    if label not in clusters:\n",
        "        clusters[label] = {'responses': [], 'ids': []}\n",
        "    clusters[label]['responses'].append(responses[i])  # Store the response\n",
        "    clusters[label]['ids'].append(ids.iloc[i])  # Store the corresponding ID\n",
        "\n",
        "############## for analysis ################\n",
        "\n",
        "# Output clusters\n",
        "for cluster_id, group in clusters.items():\n",
        "    print(f\"Group {cluster_id}: Responses: {group['responses']}, IDs: {group['ids']}\")\n",
        "\n",
        "# Convert clusters into a DataFrame\n",
        "cluster_data = []\n",
        "for cluster_id, group in clusters.items():\n",
        "    cluster_data.append({\n",
        "        'Cluster ID': cluster_id,\n",
        "        'Responses': ', '.join(group['responses']),  # Combine responses into a single string\n",
        "        'IDs': ', '.join(map(str, group['ids'])),  # Combine IDs into a single string\n",
        "        'Number of Responses': len(group['responses'])  # Add count of responses in the cluster\n",
        "    })\n",
        "\n",
        "# Create the DataFrame\n",
        "clusters_df = pd.DataFrame(cluster_data)\n",
        "clusters_df = clusters_df.sort_values(by='Number of Responses', ascending=False)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "clusters_df.to_csv('BIG_FAT_CLUSTERS.csv', index=False)\n",
        "\n",
        "df_i=clusters_df\n",
        "df_i['Label'] = df_i['Responses'].apply(ensure_list).apply(find_median_length_string)\n",
        "\n",
        "# Iterate through each row in all_res_mapping\n",
        "for index, row in df_i.iterrows():\n",
        "    ids = row['IDs']  # Get the list of IDs for this group\n",
        "    label = row['Label']  # Get the label for this group\n",
        "\n",
        "    # Iterate through each ID in the list\n",
        "    for id in ids:\n",
        "        # Find the row in qq where the 'id' column matches the current ID\n",
        "        match_index = model_df[model_df['id'] == id].index\n",
        "\n",
        "        # If a match is found, replace the 'question' column with the label\n",
        "        if not match_index.empty:\n",
        "            model_df.loc[match_index, question] = label\n",
        "\n",
        "model_df = model_df.fillna('none')\n",
        "\n",
        "# force it tf\n",
        "q2='Q2: How many ingredients would you expect this food item to contain?'\n",
        "hmmmmm = ['potato' , '#NAME?' ]\n",
        "model_df[q2] = model_df[q2].replace(hmmmmm, 'none')\n",
        "\n",
        "\n",
        "# # Save the updated qq DataFrame to a CSV file (optional)\n",
        "model_df.to_csv('/content/please_be_done.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WFeD1UIhrkBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}