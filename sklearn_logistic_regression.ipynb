{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "-2rPvWLwqH74"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # For plotting\n",
        "import numpy as np              # Linear algebra library\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/311/final_final_fr_fr.csv')\n",
        "\n",
        "# Print unique values for each column\n",
        "for column in df.columns:\n",
        "    print(f\"Unique values in {column}: {df[column].unique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdq4HT95rdnW",
        "outputId": "d24bff1b-0f7e-433c-e2cc-2845ed3270e4"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in id: [716549 715742 727333 606874 505318 605771 606929 609789 644623 626792\n",
            " 714863 602805 602526 497421 389328 289931 620148 387155 516975 630825\n",
            " 614043 521125 627780 492744 618747 607815 601609 602657 616449 501893\n",
            " 507448 606556 724326 616945 630962 715814 502799 605278 388888 519548\n",
            " 405521 607410 715795 625326 627220 634964 722500 602824 618355 611097\n",
            " 634328 607160 728522 718570 714598 601950 612478 501416 629710 502610\n",
            " 603476 600957 713423 722431 633234 148179 715845 632153 606552 738409\n",
            " 612096 743537 632896 605207 617542 719554 627427 411315 631507 627798\n",
            " 712093 627488 626493 601363 712898 603398 631012 601848 612131 408630\n",
            " 632091 601122 602871 520828 514199 389697 714430 612567 744468 502722\n",
            " 519681 631125 606119 627252 616568 603838 719735 608062 608704 520383\n",
            " 603793 603933 605519 524803 629712 627505 526131 607065 500077 633350\n",
            " 601945 629878 712766 603664 840827 604897 525422 602560 738379 622023\n",
            " 604770 608318 626757 745297 627599 520037 632189 627636 295428 228777\n",
            " 609278 614089 607517 714607 716650 497695 496858 608688 604386 628559\n",
            " 715152 604110 629010 605254 628990 625295 603415 635533 630389 714901\n",
            " 629792 629025 524441 389109 631216 738875 634669 409940 399605 721831\n",
            " 835223 712938 395712 603388 617774 289440 714503 714893 601094 618150\n",
            " 607944 629844 626482 609257 717921 181851 629041 604062 716749 633143\n",
            " 743828 520861 517019 626779 611716 193195 627530 626479 624540 625315\n",
            " 601941 723968 715806 720045 713585 611188 606726 629142 634825 613409\n",
            " 633224 738850 500470  13183 627169 229041 496344 409723 630949 413014\n",
            " 310438 627430 526510 390823 717116 712143 612538 627539 603812 740568\n",
            " 712702 519638 627800 406863 726962 628077 601693 517096 606821 455047\n",
            " 625576 747734 517064 387330 746750 629285 743488 523710 635179 313562\n",
            " 712670 635627 524485 632320 633520 606905 393424 618333 628558 613776\n",
            " 317214 628093 411781 525567 634455 498848 638838 411253 410165 608554\n",
            " 602394 743623  87783 632431 744091 519570 744484 508149 524992 413570\n",
            " 306207 634468 522205   5978 742611 631198 606150 313577 629700 630928\n",
            " 607286 607609 522890 630227 603607 617029 617462 633001 634690 630198\n",
            " 520507 635065 613209 605452  24137 518809 634646 508156 632909 414734\n",
            " 630250 411810 214145 628986 211158 507382 522877 635024 602845 519822\n",
            " 600849 605148 631600 716142 525853 835124 634764 630117 631531 714062\n",
            " 629156 498563 625319 723931 526497 630999  97339 631909 625349 632299\n",
            " 616634 616836 609116 626789 504599 632684 522967 632893 632156 603582\n",
            " 626754 298481 631583 718026 616713 602533 522345 629758 631496 491374\n",
            " 632051 409930 632975 407221 204508 618700 627792 712054 631881 716188\n",
            " 628744 604132 629807 741428 633024 631221 601983 603007 601217 629221\n",
            " 634705 630070 630006 629072 630044 626026 520303 630972 630948 626778\n",
            " 631181 633541 630098 629959 633097 605290 629869 632868 209691 600915\n",
            " 525420 414710 629985 523128 727410 616457 206487  68949 419835 745732\n",
            " 543739 410380 385307 634566 625818 631164 603130 229211 626849 634650\n",
            " 632125 626812 631109 716766 494103 628102 634683 517774 408898 603189\n",
            " 606857 523080 631014 503957 524890 717411 525311 630967 604969 524480\n",
            " 630121 629615 634977 630823 632228 504548 522477 523461 312640 854745\n",
            " 711854 739254 621555 602482 630416 505458 296589 604826 387820 603612\n",
            " 712252 742226 712771 746466 601737 631902 312320 519986 626771 407257\n",
            " 626883 727339 631448 634885 619932 601986 631141 631069 634508 206744\n",
            " 498213 627496 629659 630190 401586 629263 631678 411681 518733 630378\n",
            " 631142 519976 604991 522483 629723 387522 613484 388729 494373 630977\n",
            " 621982 605537 607073 624443 718089 622727 609617 526064 609179 627638\n",
            " 411117 634430 631711 606010 631436 633712 609556 520090 602234 741592\n",
            " 628547 501513 630929 617379 617310 495424 622787 631716 741658 620745\n",
            " 611932 635602 619892 529289 630738 524766 522148 601547 631046  66222\n",
            " 603182 632117 630209 413395 386750 617178 630128 628736]\n",
            "Unique values in Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex): [3 4 2 5 1]\n",
            "Unique values in Q2: How many ingredients would you expect this food item to contain?: ['6' '2' '5' '3' '4' '9' '10' '8' '7' '21' '11' '15' '1' '13' '12' 'none'\n",
            " '20' '25' '17' '14' '18' '23']\n",
            "Unique values in Q3: In what setting would you expect this food to be served? Please check all that apply: ['Week day lunch,At a party,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,Weekend dinner'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,Weekend dinner,At a party,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,Weekend dinner,At a party'\n",
            " 'Week day lunch'\n",
            " 'Week day dinner,Weekend lunch,Weekend dinner,At a party,Late night snack'\n",
            " 'Weekend dinner,At a party' 'Weekend lunch,Weekend dinner,At a party'\n",
            " 'Weekend lunch,Weekend dinner,At a party,Late night snack'\n",
            " 'Week day dinner,At a party'\n",
            " 'Week day dinner,Weekend dinner,At a party,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend dinner,At a party'\n",
            " 'Weekend dinner,At a party,Late night snack' 'At a party'\n",
            " 'Week day lunch,Weekend lunch,At a party'\n",
            " 'Week day lunch,Week day dinner,At a party'\n",
            " 'Week day lunch,Weekend dinner,At a party,Late night snack'\n",
            " 'Week day dinner,At a party,Late night snack'\n",
            " 'Week day dinner,Weekend dinner,At a party'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,At a party'\n",
            " 'Week day lunch,Week day dinner,At a party,Late night snack'\n",
            " 'Weekend lunch,At a party,Late night snack'\n",
            " 'Week day lunch,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,At a party,Late night snack'\n",
            " 'Week day lunch,Weekend lunch,At a party,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend dinner,At a party,Late night snack'\n",
            " 'Week day lunch,At a party' 'Week day lunch,Weekend lunch'\n",
            " 'Week day dinner,Weekend lunch,Weekend dinner,At a party'\n",
            " 'Weekend lunch,At a party' 'Weekend lunch,Weekend dinner'\n",
            " 'Week day lunch,Weekend dinner,At a party' 'Week day dinner'\n",
            " 'Week day lunch,Weekend lunch,Weekend dinner,At a party,Late night snack'\n",
            " 'Week day dinner,Weekend dinner'\n",
            " 'Week day dinner,Weekend lunch,At a party,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,Weekend dinner,Late night snack'\n",
            " 'Weekend lunch' 'Week day lunch,Weekend lunch,Late night snack'\n",
            " 'Week day lunch,Week day dinner'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch'\n",
            " 'Week day lunch,Weekend dinner'\n",
            " 'Week day dinner,Weekend dinner,Late night snack'\n",
            " 'Week day lunch,Weekend dinner,Late night snack'\n",
            " 'Weekend dinner,Late night snack'\n",
            " 'Week day lunch,Weekend lunch,Weekend dinner'\n",
            " 'Week day lunch,Weekend lunch,Weekend dinner,Late night snack'\n",
            " 'Week day dinner,Weekend lunch,Weekend dinner'\n",
            " 'Week day lunch,Week day dinner,Weekend dinner' 'Weekend dinner'\n",
            " 'Week day lunch,Weekend lunch,Weekend dinner,At a party'\n",
            " 'Week day dinner,Weekend lunch,At a party' 'At a party,Late night snack'\n",
            " 'Week day dinner,Weekend lunch' 'Week day dinner,Late night snack'\n",
            " 'Late night snack' 'Weekend lunch,Weekend dinner,Late night snack'\n",
            " 'Week day dinner,Weekend lunch,Weekend dinner,Late night snack']\n",
            "Unique values in Q4: How much would you expect to pay for one serving of this food item?: ['5.0' '10.0' '3.0' '15.0' '1.0' '20.0' '4.0' '12.0' '6.0' '30.0' '7.0'\n",
            " '8.0' '22.5' '11.5' '13.0' '6.5' '35.0' '4.99' '18.0' '2.0' '2.5' '3.5'\n",
            " '17.0' '22.0' '5.5' '16.0' '25.0' '14.0' '3.75' '4.5' '2.29' '64.0' '7.5'\n",
            " '9.0' '8.5' '11.0' '12.5' 'none' '5.99' '16.99' '57.0' '7.25' '65.0'\n",
            " '23.0' '50.0' '17.5' '45.0' '13.5' '80.0' '28.0' '14.5' '6.96' '62.0'\n",
            " '14.99' '4.29' '19.0' '16.5' '24.0' '3.95' '3.45' '4.25' '2.99' '3.99'\n",
            " '18.5' '11.99' '9.99' '16.98' '9.5' '10.5' '12.36' '13.99' '100.0' '1.5'\n",
            " '40.0' '20.99' '10.99' '12.79' '27.5' '11.02' '1.49' '32.5']\n",
            "Unique values in Q5: What movie do you think of when thinking of this food item?: ['cloudy with a chance of meatballs'\n",
            " 'all sort of american young boy movies' 'action movie' 'mamma mia' 'none'\n",
            " 'dragon' 'rick and morty' 'home alone' 'the big lebowski' 'spiderman'\n",
            " 'goodfellas' 'a goofy movie the disney movie' 'dead silence' 'la la land'\n",
            " 'harry potter' 'transformer' 'teenage mutant ninja turtles'\n",
            " 'high school musical 3' 'despicable me' 'toy story' 'the godfather'\n",
            " 'pizza' 'fast furious' 'garfield' 'spiderman no way home' 'ratatouille'\n",
            " 'five nights at freddys' 'the avengers' 'back to the future' 'godfather'\n",
            " 'mystic pizza' 'futurama' 'iron man' 'life of pi' 'any american movie'\n",
            " 'king kong' 'grown ups' 'deadpool' 'cloudy with a chance of meatball'\n",
            " 'whiplash' 'inside out' 'superbad' 'stranger things' 'air bud'\n",
            " 'a quiet place day one 2024' 'finding nemo' 'my cousin vinny'\n",
            " 'eat pray love' 'comedy' 'star wars' 'the dark knight' 'pulp fiction'\n",
            " 'fast and furious' 'the truman show' 'breaking bad' 'interstellar'\n",
            " 'rush hour' 'shrek' 'scooby doo' 'back to the future 2'\n",
            " 'princess diaries the apology scene' 'walle' 'spy kids 3d' 'squid game'\n",
            " 'running man' 'recep ivedik 2' 'venom' 'moneyball'\n",
            " 'hangover or any teen party movies which features a lot of sweet stuff'\n",
            " 'coraline' 'life if pi' 'james bond 007' 'the goofy movie'\n",
            " 'a quiet place 2' 'carryon' 'set it up' 'green book' 'cars'\n",
            " 'relaxing comedy' 'high school musical' 'zootopia' 'mario movie'\n",
            " '30 minutes or less' '21 jump street' 'free guy' 'harry poter' 'soul'\n",
            " 'life is beautiful' 'the wicked' 'harold kumar go to white castle'\n",
            " 'lord of the rings' 'the hangover' 'breaking bad not a movie'\n",
            " 'little italy' 'everything everywhere all at once' 'barbie' 'hitman'\n",
            " 'liquorice pizza' 'the dictator' 'anjaana anjaani' 'hawkeye' 'argo'\n",
            " 'transformers' 'aladdin' 'the invisible guest' 'titanic' '1001 nights'\n",
            " 'shawarma legend' 'dangal' 'spiderman into the spiderverse'\n",
            " 'slumdog millionaire' 'cleopatra' 'the kite runner' '2012' 'the boys'\n",
            " 'mission impossible' 'black hawk down' 'dictator' 'kungfu panda'\n",
            " 'alladin' 'how to lose a guy in 10 days' 'babylon' 'prince of egypt'\n",
            " 'borat' 'harold and kumar' 'gossip girl' 'monty python'\n",
            " 'khabi khushi khabi gham' 'mission impossible dead reckoning part one'\n",
            " '3 idiots' 'dune' 'parasite' 'batman the dark knight' 'the mummy'\n",
            " 'taxi driver' 'comedy movie' 'you dont mess with the zohan'\n",
            " 'spider man spider verse' 'gladiator' 'any bollywood movie'\n",
            " 'scott pilgrim vs the world' 'son of babylon'\n",
            " 'cloudy with a chance of meatballs again shawarmas dont appear in that though but hey its food'\n",
            " 'bad boys' 'the social network' 'documenary' 'chef' 'minions'\n",
            " 'the big sick' 'jurassic park' 'wicked' 'the avengers 1' 'the avenger'\n",
            " 'arcane' 'bahen' 'legend of shawama' 'forest gump' 'john wick'\n",
            " 'scott pilgrim' 'good will hunting' 'fight club' 'shawarma bowl' 'drive'\n",
            " 'lion king' 'captain of america' 'spirited away' 'japanese movies'\n",
            " 'cartoon movie' 'johnny english' 'karate kid' 'drange' 'kung fu panda'\n",
            " 'wolverine' 'jiro dreams of sushi' 'kill bill' 'your name'\n",
            " 'once upon a time in hollywood'\n",
            " 'the lord of the rings the fellowship of the ring' 'shark tale'\n",
            " 'bullet train' 'japanese' 'isle of dogs' 'little forest a japanese film'\n",
            " 'doraemon' 'naruto' 'jaws' 'the end of evangelion' 'midnight diner'\n",
            " 'anime' 'crazy rich asians' 'the perfect storm' 'the karate kid'\n",
            " 'the pacific' 'weathering with you'\n",
            " 'eternal sunshine of the spotless mind' 'cars 2' 'my neighbour totoro'\n",
            " 'oldboy' 'the proposal' 'rurouni kenshin' 'gone girl' 'my hero academia'\n",
            " 'food wars' 'your name 2016' 'liz and the blue bird' 'spirited away 2001'\n",
            " 'frozen' 'seven samurai' 'the boy and the heron' 'howls moving castle'\n",
            " 'suits' 'john wick 4' 'ghibli movies' 'bladerunner' 'a silent voice 2016'\n",
            " 'monsters inc' 'jiro dreams of sush' 'inception' 'big big wolf'\n",
            " 'wolf of wall street' 'romance movie' 'wolf of wallstreet' 'godzilla'\n",
            " 'scary movie 4 2006' 'monster house' 'the big short' 'the little mermaid'\n",
            " 'lost in translation' 'pirates of the carribeans'\n",
            " 'jiro dreams of sushi east side sushi finding nemo' 'monster inc'\n",
            " 'spirit away' 'gran turismo' 'short movie because i eat it fast'\n",
            " 'the breakfast club' 'ponyo' 'samurai jack' 'mean girls' 'girls trip'\n",
            " 'my neighbor totoro' 'anime i guess' 'passengers2016' 'madagascar'\n",
            " 'koe no katachi' 'talented' 'crayon shinchan the movie'\n",
            " 'memoirs of a geisha' 'masterchef' 'monster' 'the last samurai' 'suzume'\n",
            " 'detective conan' 'big hero 6' 'snowpiercer' 'yakuza'\n",
            " 'wizards of waverly place movie'\n",
            " 'teenage mutant ninja turtles the turtles are famous for their love of pizza'\n",
            " 'spiderman across the spiderverse' 'saving private ryan' 'mr deeds'\n",
            " 'documentary' 'murder mystery' 'the super mario bros movie' 'the ritual'\n",
            " 'breakaway' 'uncle grandpa' 'captain america' 'pirates of the caribbean'\n",
            " 'the boy and the herron' 'the grinch' 'the lego movie'\n",
            " 'futurama benders big score' 'the italian job'\n",
            " 'back to the future part 2'\n",
            " 'any kind really probably something more relaxed for leisure'\n",
            " 'american pie' 'friday' 'this is the end'\n",
            " 'ready player one there is pacman inside and pizza looks like one'\n",
            " 'the gentlemen' 'gilmore girls' 'happy gilmore' 'ratatoullie'\n",
            " 'back in action' 'die hard' 'neverending story' 'maze runner'\n",
            " 'diary of a wimpy kid rodrick rules' 'malena' 'anchorman' 'friends'\n",
            " 'terminator' 'men in black' 'the davinci code' 'crazy stupid love'\n",
            " 'the hunger games' 'luca' 'any movie set in new york' 'cartoons'\n",
            " 'waynes world' 'scific type' 'do the right thing' 'space jam' 'spongebob'\n",
            " 'diary of a wimpy kid' 'step brothers' 'horror movie'\n",
            " 'the princess diaries' 'oppenheimer'\n",
            " 'i think of movies that ive watch with either my friends or my siblings'\n",
            " 'the barbie movie' 'pursuit of happyness' 'middle east movie' 'drishyam'\n",
            " 'us' 'shrek2' 'nosferatu' 'alien' 'indiana jones' 'angry birds'\n",
            " 'the road to fallujah' 'kingdom of heaven' 'lawrence of arabia'\n",
            " 'time of happiness' 'middle eastern movies' 'hangover' 'coco' '11sep'\n",
            " '13 hours' 'murder on the orient express' 'three idiots' 'shazam'\n",
            " 'the lion king' 'aladdin idk' 'deadpool and wolverine' 'mulan' 'mandoob'\n",
            " 'the prince of egypt' 'indian movie' 'south park the end of obesity'\n",
            " 'yeh jawaani hai deewani' 'dabba' 'cheech chong' 'shawshank redemption'\n",
            " 'mad max' 'taken 2' 'ferris buellers day off' 'the dora movie'\n",
            " 'the whale' 'chinese zodiac' 'sonic the hedgehog' 'bollywood'\n",
            " 'middle eastern movie' 'burnt' 'rush hour 3' 'scary movie' 'the menu'\n",
            " 'the emoji movie' 'rush hour 2' 'a silent voice' 'pokemon' 'house md'\n",
            " 'the fast and the furious tokyo drift' 'who am i jackie chan'\n",
            " 'japanese movies food documentaries' 'kodoku no gurume' 'gi joe'\n",
            " 'penguins of madagascar' 'japanese movie' 'shrek3' 'aquaman' '47 ronin'\n",
            " 'barbie 2021' 'pengnuins of madagascar' '9' 'james bond' 'wags'\n",
            " 'notting hill' 'made in abyss dawn of the deep soul' 'meitantei konan'\n",
            " 'shangchi' 'billions tv show' 'madagascar 2' 'ip man' 'django unchained'\n",
            " 'the samurai' 'shangchi and the legend of the ten rings' 'karate kid ii'\n",
            " '7 samurai' 'casablanca' 'pacific rim' 'train to busan' 'love hard'\n",
            " 'shogun' 'the room' 'fallen angels' 'totoro'\n",
            " 'shinchan the movie chounouryoku daikessen tobe tobe temakizushi'\n",
            " 'shanghai noon' 'one piece' 'turning red' 'blade runner'\n",
            " 'fast furious tokyo drift' 'lucy' 'hunger' 'the intern' 'the meg 2018'\n",
            " 'rush hour series' 'ultraman rising' 'pearl harbour' 'good time'\n",
            " 'who am i 2005' 'memory of a geisha' 'last samurai' 'city hunter'\n",
            " 'alitathe warrior' 'haikyu' 'the spy next door' 'shang chi'\n",
            " 'breakfast club' 'heretic' 'crayon shinchan' 'anime movie' 'cars 3'\n",
            " 'east side sushi' 'shang chi the legend of ten rings'\n",
            " 'chandni chowk to china' 'midnight dinner' 'kikis delivery service'\n",
            " 'romantic movies' 'one piece film gold']\n",
            "Unique values in Q6: What drink would you pair with this food item?: ['coke' 'soda' 'ice tea' 'lemonade' 'red wine' 'sprite' 'water' 'soup'\n",
            " 'root beer' '7up' 'none' 'orange fanta' 'mountain dew' 'diet pepsi'\n",
            " 'juice' 'dr pepper' 'soft drink' 'pop' 'ginger ale' 'milk' 'boba'\n",
            " 'lemon tea' 'ayran' 'lassi' 'carbonated drink' 'the avengers' 'jarritos'\n",
            " 'oolong tea' 'sake' 'green tea' 'calpis' 'barley tea' 'hot tea'\n",
            " 'fish sauce' 'matcha' 'champagne' 'crush' 'pineapple fanta' 'canada dry'\n",
            " 'gatorade' 'sprindrift' 'martini cocktail' 'smoothie' 'laban' 'yogurt'\n",
            " 'coffee' 'barbican' 'leban' 'fruit tea' 'dairy' 'powerade' 'ramune'\n",
            " 'mango pulp' 'diet brisk' 'yakult' 'saporo' 'soy sauce' 'baijiu'\n",
            " 'any tea']\n",
            "Unique values in Q7: When you think about this food item, who does it remind you of?: ['Friends' 'Friends,Teachers,Strangers' 'Siblings,Friends,Teachers'\n",
            " 'Siblings,Friends' 'none' 'Parents,Siblings,Friends' 'Parents,Friends'\n",
            " 'Friends,Strangers' 'Parents,Siblings,Friends,Strangers' 'Parents'\n",
            " 'Parents,Siblings,Friends,Teachers' 'Friends,Teachers' 'Teachers'\n",
            " 'Parents,Friends,Teachers' 'Siblings,Teachers' 'Teachers,Strangers'\n",
            " 'Siblings' 'Siblings,Friends,Strangers' 'Parents,Siblings,Teachers'\n",
            " 'Parents,Siblings' 'Strangers' 'Siblings,Friends,Teachers,Strangers'\n",
            " 'Parents,Friends,Teachers,Strangers'\n",
            " 'Parents,Siblings,Friends,Teachers,Strangers' 'Parents,Teachers'\n",
            " 'Parents,Friends,Strangers' 'Parents,Siblings,Strangers'\n",
            " 'Parents,Strangers' 'Siblings,Strangers' 'Parents,Teachers,Strangers']\n",
            "Unique values in Q8: How much hot sauce would you add to this food item?: ['A little (mild)' 'none' 'A moderate amount (medium)'\n",
            " 'I will have some of this food item with my hot sauce' 'A lot (hot)']\n",
            "Unique values in Label: ['Pizza' 'Shawarma' 'Sushi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def vectorized_one_hot(answers, attributes, attribute_to_index):\n",
        "#     indices_list = [\n",
        "#         [attribute_to_index[attr] for attr in ans if attr in attribute_to_index]\n",
        "#         for ans in answers\n",
        "#     ]\n",
        "\n",
        "#     # Create a 2D array filled with zeros\n",
        "#     one_hot_matrix = np.zeros((len(answers), len(attributes)), dtype=int)\n",
        "\n",
        "#     # Set appropriate positions to 1\n",
        "#     for i, indices in enumerate(indices_list):\n",
        "#         one_hot_matrix[i, indices] = 1\n",
        "\n",
        "#     return one_hot_matrix\n",
        "\n",
        "\n",
        "\n",
        "# q1 = \"\"\"Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)\"\"\"\n",
        "# q2='Q2: How many ingredients would you expect this food item to contain?'\n",
        "# q3='Q3: In what setting would you expect this food to be served? Please check all that apply'\n",
        "# q4='Q4: How much would you expect to pay for one serving of this food item?'\n",
        "# q5='Q5: What movie do you think of when thinking of this food item?'\n",
        "# q6='Q6: What drink would you pair with this food item?'\n",
        "# q7='Q7: When you think about this food item, who does it remind you of?'\n",
        "# q8='Q8: How much hot sauce would you add to this food item?'\n",
        "# t='Label'\n",
        "\n",
        "\n",
        "# df = df[df[q2] != 'none']\n",
        "# df = df[df[q4] != 'none']\n",
        "\n",
        "# # # Data cleaning\n",
        "# # df[q2] = df[q2].replace(\"none\", '0').astype(str)\n",
        "# # df[q4] = df[q4].replace(\"none\", '0').astype(str)\n",
        "\n",
        "# # one hot codings\n",
        "# q1_options = [1,2,3,4,5]\n",
        "# q3_options = ['none','Week day lunch','Week day dinner','Weekend lunch','Weekend dinner','At a party', 'Late night snack']\n",
        "# q7_options = ['Parents','Siblings','Friends', 'Teachers', 'Strangers' , 'none']\n",
        "# q8_options = ['I will have some of this food item with my hot sauce', 'A lot (hot)', 'A moderate amount (medium)', 'A little (mild)', 'none']\n",
        "\n",
        "# # one_hot = pd.get_dummies(df[q3], prefix=\"HotSauce\")\n",
        "# # print(one_hot)\n",
        "\n",
        "# q3_attribute_to_index = {attr: idx for idx, attr in enumerate(q3_options)}\n",
        "# q3_ans = df[q3].astype(str).tolist()\n",
        "# q3_answers = [ans.split(\",\") for ans in q3_ans]\n",
        "# q3_hot = vectorized_one_hot(q3_answers, q3_options, q3_attribute_to_index)\n",
        "# print(q3_hot, '\\n')\n",
        "\n",
        "# q7_attribute_to_index = {attr: idx for idx, attr in enumerate(q7_options)}\n",
        "# q7_ans = df[q7].astype(str).tolist()\n",
        "# q7_answers = [ans.split(\",\") for ans in q7_ans]\n",
        "# q7_hot = vectorized_one_hot(q7_answers, q7_options, q7_attribute_to_index)\n",
        "# print(q7_hot, '\\n')\n",
        "\n",
        "# q8_attribute_to_index = {attr: idx for idx, attr in enumerate(q8_options)}\n",
        "# q8_ans = df[q8].astype(str).tolist()\n",
        "# q8_answers = [ans.split(\",\") for ans in q8_ans]\n",
        "# q8_hot = vectorized_one_hot(q8_answers, q8_options, q8_attribute_to_index)\n",
        "# print(q8_hot, '\\n')"
      ],
      "metadata": {
        "id": "nJSAafZU1j3y"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# YAY WORKS IDK HOW\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.preprocessing import (StandardScaler, LabelEncoder,\n",
        "                                 FunctionTransformer)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Your vectorized_one_hot function\n",
        "def vectorized_one_hot(answers, options, attribute_to_index):\n",
        "    num_samples = len(answers)\n",
        "    num_attributes = len(options)\n",
        "    one_hot_matrix = np.zeros((num_samples, num_attributes))\n",
        "\n",
        "    for i, ans_list in enumerate(answers):\n",
        "        for ans in ans_list:\n",
        "            ans = ans.strip()\n",
        "            if ans in attribute_to_index:\n",
        "                one_hot_matrix[i, attribute_to_index[ans]] = 1\n",
        "    return one_hot_matrix\n",
        "\n",
        "# Define question columns\n",
        "questions = [q1, q2, q3, q4, q5, q6, q7, q8] = [\n",
        "    \"Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)\",\n",
        "    \"Q2: How many ingredients would you expect this food item to contain?\",\n",
        "    \"Q3: In what setting would you expect this food to be served? Please check all that apply\",\n",
        "    \"Q4: How much would you expect to pay for one serving of this food item?\",\n",
        "    \"Q5: What movie do you think of when thinking of this food item?\",\n",
        "    \"Q6: What drink would you pair with this food item?\",\n",
        "    \"Q7: When you think about this food item, who does it remind you of?\",\n",
        "    \"Q8: How much hot sauce would you add to this food item?\"\n",
        "]\n",
        "t = 'Label'\n",
        "\n",
        "\n",
        "# Data cleaning\n",
        "# df[q2] = df[q2].replace(\"none\", np.nan)\n",
        "# df[q4] = df[q4].replace(\"none\", np.nan)\n",
        "\n",
        "# np.nan\n",
        "# df = df[df[q2] != 'none']\n",
        "# df = df[df[q4] != 'none']\n",
        "# df[q1] = pd.to_numeric(df[q1])\n",
        "# df[q2] = pd.to_numeric(df[q2])\n",
        "# df[q4] = pd.to_numeric(df[q4])\n",
        "\n",
        "df[q1] = pd.to_numeric(df[q1], errors='coerce')\n",
        "df[q2] = pd.to_numeric(df[q2], errors='coerce')\n",
        "df[q4] = pd.to_numeric(df[q4], errors='coerce')\n",
        "\n",
        "# Create missing indicators before imputation\n",
        "for col in [q1, q2, q4]:\n",
        "    df[f'{col}_missing'] = df[col].isna().astype(int)\n",
        "\n",
        "# Fill numerical NaNs with median (now including missing indicators)\n",
        "df[q1].fillna(df[q1].median(), inplace=True)\n",
        "df[q2].fillna(df[q2].median(), inplace=True)\n",
        "df[q4].fillna(df[q4].median(), inplace=True)\n",
        "\n",
        "# Define options for categorical columns\n",
        "q3_options = ['none','Week day lunch','Week day dinner','Weekend lunch',\n",
        "             'Weekend dinner','At a party', 'Late night snack']\n",
        "q7_options = ['Parents','Siblings','Friends', 'Teachers', 'Strangers', 'none']\n",
        "q8_options = ['I will have some of this food item with my hot sauce',\n",
        "             'A lot (hot)', 'A moderate amount (medium)', 'A little (mild)', 'none']\n",
        "\n",
        "# Create attribute to index mappings\n",
        "q3_attribute_to_index = {attr: idx for idx, attr in enumerate(q3_options)}\n",
        "q7_attribute_to_index = {attr: idx for idx, attr in enumerate(q7_options)}\n",
        "q8_attribute_to_index = {attr: idx for idx, attr in enumerate(q8_options)}\n",
        "\n",
        "# Manual feature engineering approach\n",
        "numerical_features = df[[q1, q2, q4]].values\n",
        "\n",
        "# Convert DataFrame columns to lists before splitting\n",
        "q3_answers = [ans.split(\",\") for ans in df[q3].astype(str).tolist()]\n",
        "q7_answers = [ans.split(\",\") for ans in df[q7].astype(str).tolist()]\n",
        "q8_answers = [ans.split(\",\") for ans in df[q8].astype(str).tolist()]\n",
        "\n",
        "q3_hot = vectorized_one_hot(q3_answers, q3_options, q3_attribute_to_index)\n",
        "q7_hot = vectorized_one_hot(q7_answers, q7_options, q7_attribute_to_index)\n",
        "q8_hot = vectorized_one_hot(q8_answers, q8_options, q8_attribute_to_index)\n",
        "\n",
        "# Frequency encoding\n",
        "q5_encoded = df[q5].map(df[q5].value_counts(normalize=True)).values.reshape(-1, 1)\n",
        "q6_encoded = df[q6].map(df[q6].value_counts(normalize=True)).values.reshape(-1, 1)\n",
        "\n",
        "# Combine features\n",
        "X = np.hstack([\n",
        "    numerical_features,\n",
        "    q3_hot,\n",
        "    q7_hot,\n",
        "    q8_hot,\n",
        "    q5_encoded,\n",
        "    q6_encoded\n",
        "])\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[t])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Manual model\n",
        "manual_model = LogisticRegression(\n",
        "    multi_class=\"multinomial\",\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=1000\n",
        ")\n",
        "manual_model.fit(X_train, y_train)\n",
        "print(\"Manual Accuracy:\", manual_model.score(X_test, y_test))\n",
        "\n",
        "# Pipeline approach\n",
        "def create_vectorized_one_hot(options, attribute_to_index):\n",
        "    def transformer(X):\n",
        "        X = pd.DataFrame(X).iloc[:, 0]  # Convert input to Series\n",
        "        answers = [ans.split(\",\") for ans in X.astype(str)]\n",
        "        return vectorized_one_hot(answers, options, attribute_to_index)\n",
        "    return FunctionTransformer(transformer)\n",
        "\n",
        "def create_frequency_encoder(col):\n",
        "    def transformer(X):\n",
        "        X = pd.DataFrame(X).iloc[:, 0]  # Convert input to Series\n",
        "        return X.map(col.value_counts(normalize=True)).values.reshape(-1, 1)\n",
        "    return FunctionTransformer(transformer)\n",
        "\n",
        "def add_missing_indicator(X):\n",
        "    return np.where(X.isnull(), 1, 0).reshape(-1, 1)\n",
        "\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', ColumnTransformer([\n",
        "        # ('num', StandardScaler(), [q1, q2, q4]),\n",
        "        ('num', Pipeline([\n",
        "          ('impute', SimpleImputer(strategy='median', fill_value=0)),\n",
        "          ('scale', StandardScaler())\n",
        "        ]), [q1, q2, q4]),\n",
        "\n",
        "        # Missing indicators for numericals\n",
        "        ('q1_missing', FunctionTransformer(add_missing_indicator), [q1]),\n",
        "        ('q2_missing', FunctionTransformer(add_missing_indicator), [q2]),\n",
        "        ('q4_missing', FunctionTransformer(add_missing_indicator), [q4]),\n",
        "\n",
        "\n",
        "        ('q3', create_vectorized_one_hot(q3_options, q3_attribute_to_index), [q3]),\n",
        "        ('q7', create_vectorized_one_hot(q7_options, q7_attribute_to_index), [q7]),\n",
        "        ('q8', create_vectorized_one_hot(q8_options, q8_attribute_to_index), [q8]),\n",
        "        ('q5', create_frequency_encoder(df[q5]), [q5]),\n",
        "        ('q6', create_frequency_encoder(df[q6]), [q6])\n",
        "    ], remainder='drop')),\n",
        "    ('classifier', LogisticRegression(\n",
        "        multi_class='multinomial',\n",
        "        solver='lbfgs',\n",
        "        max_iter=1000,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Fit pipeline\n",
        "pipeline.fit(df[questions], y)\n",
        "\n",
        "# Evaluate pipeline\n",
        "print(\"Pipeline Accuracy:\", pipeline.score(df[questions], y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keiWhaajAFi8",
        "outputId": "ac915a97-730c-431f-8b7a-5cf94823ca66"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-5c6c531eaf52>:62: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[q1].fillna(df[q1].median(), inplace=True)\n",
            "<ipython-input-82-5c6c531eaf52>:63: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[q2].fillna(df[q2].median(), inplace=True)\n",
            "<ipython-input-82-5c6c531eaf52>:64: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[q4].fillna(df[q4].median(), inplace=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Accuracy: 0.790273556231003\n",
            "Pipeline Accuracy: 0.8071776155717761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning hyperparams"
      ],
      "metadata": {
        "id": "b-5Eg3bSgK62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 1. Keep your EXACT pipeline definition\n",
        "\n",
        "# from sklearn.base import clone\n",
        "\n",
        "# # 2. Split using original DataFrame columns (preserves your pipeline's expectations)\n",
        "# X = df[questions]  # Preserves Q1, Q2,...Q8 as-is\n",
        "# y = le.fit_transform(df[t])\n",
        "\n",
        "# # 3. Train/Val/Test Split (80/10/10)\n",
        "# X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "#     X, y, test_size=0.1, random_state=42, stratify=y\n",
        "# )\n",
        "# X_train, X_val, y_train, y_val = train_test_split(\n",
        "#     X_trainval, y_trainval, test_size=0.111, random_state=42, stratify=y_trainval  # 0.111*0.9=0.1\n",
        "# )\n",
        "\n",
        "# # 4. Hyperparameter Tuning (using validation set)\n",
        "# param_grid = {\n",
        "#     'classifier__C': [0.1, 1, 10],\n",
        "#     'classifier__penalty': ['l2']\n",
        "# }\n",
        "\n",
        "# search = GridSearchCV(\n",
        "#     pipeline,  # Your original pipeline\n",
        "#     param_grid,\n",
        "#     cv=3,\n",
        "#     scoring='accuracy',\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# # 5. Train on training set ONLY\n",
        "# search.fit(X_train, y_train)\n",
        "\n",
        "# # 6. Validate on validation set\n",
        "# val_acc = search.score(X_val, y_val)\n",
        "# print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# # 7. Final test on untouched test set\n",
        "# test_acc = search.score(X_test, y_test)\n",
        "# print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# # 8. (Optional) Retrain on train+val with best params\n",
        "# best_pipeline = clone(search.best_estimator_)\n",
        "# best_pipeline.fit(X_trainval, y_trainval)\n",
        "\n",
        "\n",
        "# # 3. Evaluate on all sets\n",
        "# def evaluate_model(model, X, y, set_name):\n",
        "#     y_pred = model.predict(X)\n",
        "#     acc = accuracy_score(y, y_pred)\n",
        "#     report = classification_report(y, y_pred, output_dict=True)\n",
        "#     return {\n",
        "#         'set': set_name,\n",
        "#         'accuracy': acc,\n",
        "#         **report['weighted avg']  # Unpacks precision/recall/f1\n",
        "#     }\n",
        "\n",
        "# best_model=best_pipeline\n",
        "\n",
        "# # 4. Generate reports for all sets\n",
        "# results = [\n",
        "#     evaluate_model(best_model, X_train, t_train, 'Training'),\n",
        "#     evaluate_model(best_model, X_val, t_val, 'Validation'),\n",
        "#     evaluate_model(best_model, X_test, t_test, 'Test')\n",
        "# ]\n",
        "\n",
        "# # 5. Create combined report\n",
        "# combined_report = pd.DataFrame(results).set_index('set')\n",
        "# combined_report.insert(0, 'best_params', str(grid_search.best_params_))\n",
        "\n",
        "# # 6. Print formatted results\n",
        "# print(\"=\"*80)\n",
        "# print(f\"🔥 Best Parameters: {grid_search.best_params_}\")\n",
        "# print(\"=\"*80)\n",
        "# print(combined_report[['accuracy', 'precision', 'recall', 'f1-score']])\n",
        "# print(\"=\"*80)\n",
        "# print(\"📝 Detailed Classification Reports:\")\n",
        "# for r in results:\n",
        "#     print(f\"\\n⭐ {r['set'].upper()} SET:\")\n",
        "#     print(classification_report(t_true, best_model.predict(X_sets[r['set']])))\n"
      ],
      "metadata": {
        "id": "ggwghnh_nabY"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import clone\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Keep your EXACT pipeline definition\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', ColumnTransformer([\n",
        "        ('num', Pipeline([\n",
        "            ('impute', SimpleImputer(strategy='median')),\n",
        "            ('scale', StandardScaler())\n",
        "        ]), [q1, q2, q4]),\n",
        "\n",
        "        ('q1_missing', FunctionTransformer(add_missing_indicator), [q1]),\n",
        "        ('q2_missing', FunctionTransformer(add_missing_indicator), [q2]),\n",
        "        ('q4_missing', FunctionTransformer(add_missing_indicator), [q4]),\n",
        "\n",
        "        ('q3', create_vectorized_one_hot(q3_options, q3_attribute_to_index), [q3]),\n",
        "        ('q7', create_vectorized_one_hot(q7_options, q7_attribute_to_index), [q7]),\n",
        "        ('q8', create_vectorized_one_hot(q8_options, q8_attribute_to_index), [q8]),\n",
        "        ('q5', create_frequency_encoder(df[q5]), [q5]),\n",
        "        ('q6', create_frequency_encoder(df[q6]), [q6])\n",
        "    ], remainder='drop')),\n",
        "    ('classifier', LogisticRegression(\n",
        "        multi_class='multinomial',\n",
        "        solver='lbfgs',\n",
        "        max_iter=1000,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 2. Prepare data\n",
        "X = df[questions]  # Preserves original column names\n",
        "y = le.fit_transform(df[t])\n",
        "\n",
        "# 3. Train/Val/Test Split (80/10/10)\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=42, stratify=y\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval,\n",
        "    test_size=0.111,  # 0.111 * 0.9 ≈ 0.1\n",
        "    random_state=42,\n",
        "    stratify=y_trainval\n",
        ")\n",
        "\n",
        "# 4. Hyperparameter Tuning\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__penalty': ['l2']\n",
        "}\n",
        "\n",
        "search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Get best model and retrain on train+val\n",
        "best_pipeline = clone(search.best_estimator_)\n",
        "best_pipeline.fit(X_trainval, y_trainval)\n",
        "\n",
        "# 6. Evaluation function\n",
        "def evaluate_model(model, X, y_true, set_name):\n",
        "    y_pred = model.predict(X)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    report = classification_report(y_true, y_pred, output_dict=True)\n",
        "    return {\n",
        "        'set': set_name,\n",
        "        'accuracy': acc,\n",
        "        'precision': report['weighted avg']['precision'],\n",
        "        'recall': report['weighted avg']['recall'],\n",
        "        'f1-score': report['weighted avg']['f1-score']\n",
        "    }\n",
        "\n",
        "# 7. Generate comprehensive reports\n",
        "results = [\n",
        "    evaluate_model(best_pipeline, X_train, y_train, 'Training'),\n",
        "    evaluate_model(best_pipeline, X_val, y_val, 'Validation'),\n",
        "    evaluate_model(best_pipeline, X_test, y_test, 'Test')\n",
        "]\n",
        "\n",
        "# 8. Create combined report\n",
        "combined_report = pd.DataFrame(results).set_index('set')\n",
        "combined_report.insert(0, 'best_params', str(search.best_params_))\n",
        "\n",
        "# 9. Print beautiful output\n",
        "print(\"=\"*80)\n",
        "print(f\"🔥 Best Parameters: {search.best_params_}\")\n",
        "print(\"=\"*80)\n",
        "print(combined_report[['accuracy', 'precision', 'recall', 'f1-score']])\n",
        "print(\"=\"*80)\n",
        "print(\"📝 Detailed Classification Reports:\")\n",
        "\n",
        "for set_name, X_set, y_set in [('Training', X_train, y_train),\n",
        "                               ('Validation', X_val, y_val),\n",
        "                               ('Test', X_test, y_test)]:\n",
        "    print(f\"\\n⭐ {set_name.upper()} SET:\")\n",
        "    print(classification_report(y_set, best_pipeline.predict(X_set)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fZ2q9-XyzP9",
        "outputId": "9a53f660-72e9-4ef4-e66a-1dba7697e62c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "🔥 Best Parameters: {'classifier__C': 10, 'classifier__penalty': 'l2'}\n",
            "================================================================================\n",
            "            accuracy  precision    recall  f1-score\n",
            "set                                                \n",
            "Training    0.824962   0.826901  0.824962  0.824484\n",
            "Validation  0.781818   0.780830  0.781818  0.781024\n",
            "Test        0.775758   0.777339  0.775758  0.774837\n",
            "================================================================================\n",
            "📝 Detailed Classification Reports:\n",
            "\n",
            "⭐ TRAINING SET:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.83       438\n",
            "           1       0.87      0.84      0.85       438\n",
            "           2       0.83      0.76      0.79       438\n",
            "\n",
            "    accuracy                           0.82      1314\n",
            "   macro avg       0.83      0.82      0.82      1314\n",
            "weighted avg       0.83      0.82      0.82      1314\n",
            "\n",
            "\n",
            "⭐ VALIDATION SET:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.78      0.79        55\n",
            "           1       0.81      0.85      0.83        55\n",
            "           2       0.74      0.71      0.72        55\n",
            "\n",
            "    accuracy                           0.78       165\n",
            "   macro avg       0.78      0.78      0.78       165\n",
            "weighted avg       0.78      0.78      0.78       165\n",
            "\n",
            "\n",
            "⭐ TEST SET:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.80      0.77        55\n",
            "           1       0.81      0.84      0.82        55\n",
            "           2       0.79      0.69      0.74        55\n",
            "\n",
            "    accuracy                           0.78       165\n",
            "   macro avg       0.78      0.78      0.77       165\n",
            "weighted avg       0.78      0.78      0.77       165\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation hypertuning using grid search and logistic"
      ],
      "metadata": {
        "id": "N00hYUKVq0nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your existing manual feature engineering\n",
        "X = np.hstack([\n",
        "    numerical_features,  # Q1,Q2,Q4\n",
        "    q3_hot,             # Q3 one-hot\n",
        "    q7_hot,             # Q7 one-hot\n",
        "    q8_hot,             # Q8 one-hot\n",
        "    q5_encoded,         # Q5 frequency\n",
        "    q6_encoded          # Q6 frequency\n",
        "])\n",
        "y = le.fit_transform(df[t])\n",
        "\n",
        "# First split: 80% train+val, 20% test (UNTOUCHED final test set)\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Second split: 60% train, 20% validation (of original data)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval,\n",
        "    test_size=0.25,  # 0.25 * 0.8 = 0.2 of original\n",
        "    random_state=42,\n",
        "    stratify=y_trainval\n",
        ")\n",
        "\n",
        "# Now you have:\n",
        "# - X_train, y_train (60% of original)\n",
        "# - X_val, y_val (20% of original)\n",
        "# - X_test, y_test (20% of original - ONLY FOR FINAL EVAL)\n",
        "\n",
        "# Train model on training set\n",
        "manual_model = LogisticRegression(\n",
        "    multi_class=\"multinomial\",\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=1000\n",
        ")\n",
        "manual_model.fit(X_train, y_train)\n",
        "\n",
        "# 1. Check validation performance\n",
        "val_acc = manual_model.score(X_val, y_val)\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# 2. Only after you're satisfied, evaluate on test set\n",
        "test_acc = manual_model.score(X_test, y_test)\n",
        "print(f\"Final Test Accuracy: {val_acc:.4f} → {test_acc:.4f}\")\n",
        "\n",
        "# (Optional) Retrain on train+val for final model\n",
        "final_model = LogisticRegression(\n",
        "    multi_class=\"multinomial\",\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=1000\n",
        ")\n",
        "final_model.fit(X_trainval, y_trainval)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "8L453BjCqktU",
        "outputId": "172bf651-f7c8-443a-af91-8510f535d444"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7903\n",
            "Final Test Accuracy: 0.7903 → 0.7660\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000, multi_class='multinomial')"
            ],
            "text/html": [
              "<style>#sk-container-id-7 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-7 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-7 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-7 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-7 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-7 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-7 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-7 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-7 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-7 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-7 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-7 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-7 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-7 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-7 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-7 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;multinomial&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" checked><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;multinomial&#x27;)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization\n",
        "    'penalty': ['l2'],\n",
        "    'max_iter': [1000]\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning\n",
        "search = GridSearchCV(\n",
        "    LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\"),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_model = search.best_estimator_\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Get label mapping (add this before your evaluation)\n",
        "label_mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
        "reverse_mapping = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "# Function to generate comprehensive metrics (updated)\n",
        "def evaluate_model(model, X, y, set_name):\n",
        "    y_pred = model.predict(X)\n",
        "    metrics = {\n",
        "        'set': set_name,\n",
        "        'accuracy': accuracy_score(y, y_pred),\n",
        "        'best_params': str(search.best_params_)\n",
        "    }\n",
        "\n",
        "    # Generate and store both raw and readable reports\n",
        "    report = classification_report(y, y_pred, output_dict=True)\n",
        "    readable_report = classification_report(y, y_pred, target_names=le.classes_)\n",
        "\n",
        "    for metric in ['precision', 'recall', 'f1-score']:\n",
        "        metrics.update({\n",
        "            f'{metric}_macro': report['macro avg'][metric],\n",
        "            f'{metric}_weighted': report['weighted avg'][metric]\n",
        "        })\n",
        "    return metrics, readable_report\n",
        "\n",
        "# [Keep all your existing code until the report printing section]\n",
        "\n",
        "# Print formatted results with label mapping\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"🔥 Best Parameters: {search.best_params_}\")\n",
        "print(f\"🔤 Label Mapping: {label_mapping}\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n📊 Performance Summary:\")\n",
        "print(report_df[['accuracy', 'precision_weighted', 'recall_weighted', 'f1-score_weighted']])\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Print detailed classification reports with actual labels\n",
        "print(\"\\n📝 Detailed Classification Reports (with label names):\")\n",
        "for set_name, report in detailed_reports.items():\n",
        "    print(f\"\\n⭐ {set_name.upper()} SET:\")\n",
        "    print(report)\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Optional: Add prediction examples\n",
        "sample_preds = best_model.predict(X_val[:3])\n",
        "print(\"\\n🔮 Sample Predictions:\")\n",
        "for i, pred in enumerate(sample_preds):\n",
        "    print(f\"  {i+1}. Predicted: {reverse_mapping[pred]} (Class {pred})\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhUrrw-AqlLO",
        "outputId": "5375a2eb-b76d-4b29-ce5c-9c679add2a03"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "🔥 Best Parameters: {'C': 100, 'max_iter': 1000, 'penalty': 'l2'}\n",
            "🔤 Label Mapping: {'Pizza': 0, 'Shawarma': 1, 'Sushi': 2}\n",
            "================================================================================\n",
            "\n",
            "📊 Performance Summary:\n",
            "            accuracy  precision_weighted  recall_weighted  f1-score_weighted\n",
            "set                                                                         \n",
            "Training    0.796923            0.798997         0.796923           0.794924\n",
            "Validation  0.778462            0.779770         0.778462           0.778133\n",
            "Test        0.788344            0.788088         0.788344           0.787910\n",
            "\n",
            "================================================================================\n",
            "\n",
            "📝 Detailed Classification Reports (with label names):\n",
            "\n",
            "⭐ TRAINING SET:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.86      0.81       325\n",
            "           1       0.82      0.85      0.84       326\n",
            "           2       0.81      0.68      0.74       324\n",
            "\n",
            "    accuracy                           0.80       975\n",
            "   macro avg       0.80      0.80      0.79       975\n",
            "weighted avg       0.80      0.80      0.79       975\n",
            "\n",
            "\n",
            "⭐ VALIDATION SET:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.83      0.79       109\n",
            "           1       0.82      0.79      0.80       108\n",
            "           2       0.77      0.72      0.75       108\n",
            "\n",
            "    accuracy                           0.78       325\n",
            "   macro avg       0.78      0.78      0.78       325\n",
            "weighted avg       0.78      0.78      0.78       325\n",
            "\n",
            "\n",
            "⭐ TEST SET:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.83      0.81       109\n",
            "           1       0.81      0.80      0.81       109\n",
            "           2       0.76      0.73      0.75       108\n",
            "\n",
            "    accuracy                           0.79       326\n",
            "   macro avg       0.79      0.79      0.79       326\n",
            "weighted avg       0.79      0.79      0.79       326\n",
            "\n",
            "================================================================================\n",
            "\n",
            "🔮 Sample Predictions:\n",
            "  1. Predicted: Pizza (Class 0)\n",
            "  2. Predicted: Pizza (Class 0)\n",
            "  3. Predicted: Shawarma (Class 1)\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}