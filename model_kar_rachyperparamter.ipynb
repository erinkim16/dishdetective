{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k5B-30SICSV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "\n",
        "# credits: https://github.com/AssemblyAI-Community/Machine-Learning-From-Scratch/blob/main/05%20Random%20Forests/train.py\n",
        "# chatgpt, deep seek, copilot\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "class DecisionTree:\n",
        "    def __init__(self, criterion=\"gini\", max_depth=30, min_samples_split=10, min_samples_leaf=1, n_features=None):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.n_features = n_features\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_feats = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # check the stopping criteria\n",
        "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False) if self.n_features else np.arange(n_feats)\n",
        "\n",
        "        # find the best split\n",
        "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "        # create child nodes\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
        "        return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "    def _best_split(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "\n",
        "            for thr in thresholds:\n",
        "                # calculate the information gain\n",
        "                gain = self._information_gain(y, X_column, thr)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = thr\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "        # parent entropy\n",
        "        parent_entropy = self._entropy(y)\n",
        "\n",
        "        # create children\n",
        "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        # calculate the weighted avg. entropy of children\n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
        "\n",
        "        # calculate the IG\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        hist = np.bincount(y)\n",
        "        ps = hist / len(y)\n",
        "        return -np.sum([p * np.log(p) for p in ps if p > 0])\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        if len(y) == 0:\n",
        "            raise ValueError(\"y is empty, cannot determine most common label!\")\n",
        "\n",
        "        counter = Counter(y)\n",
        "\n",
        "        # Check if the counter is empty\n",
        "        if not counter:\n",
        "            return None  # You can return a default value if needed, but in general this should not happen.\n",
        "\n",
        "        # Get the most common label\n",
        "        most_common = counter.most_common(1)[0][0]\n",
        "        return most_common\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "\n",
        "\n",
        "\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees=100, criterion=\"gini\", max_depth=30, min_samples_split=10, min_samples_leaf=1, n_features=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.n_features = n_features\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        assert len(X) > 0 and len(y) > 0, \"Input data is empty!\"\n",
        "        self.trees = []\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        # Calculate actual number of features to use\n",
        "        if isinstance(self.n_features, str):\n",
        "            if self.n_features == 'sqrt':\n",
        "                n_feature = int(math.sqrt(n_features))\n",
        "            elif self.n_features == 'log2':\n",
        "                n_feature = int(math.log2(n_features))\n",
        "            else:\n",
        "                n_feature = None\n",
        "        else:\n",
        "            n_feature = self.n_features\n",
        "\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(\n",
        "                criterion=self.criterion,\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                min_samples_leaf=self.min_samples_leaf,  # Pass min_samples_leaf\n",
        "                n_features=n_feature  # Pass the computed integer value for feature selection\n",
        "            )\n",
        "            X_sample, y_sample = self._bootstrap_samples(X, y)\n",
        "\n",
        "            assert len(y_sample) > 0, \"y_train is empty after bootstrapping!\"\n",
        "\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def _bootstrap_samples(self, X, y):\n",
        "\n",
        "        print(f\"X_train shape: {X.shape}, y_train shape: {y.shape}\")\n",
        "\n",
        "        n_samples = X.shape[0]\n",
        "        if n_samples == 0:\n",
        "            return X, y  # Return original if no samples\n",
        "        idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        return X[idxs], y[idxs]\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        if len(y) == 0:\n",
        "            raise ValueError(\"y is empty, cannot determine most common label!\")\n",
        "\n",
        "        counter = Counter(y)\n",
        "\n",
        "        # Check if the counter is empty\n",
        "        if not counter:\n",
        "            return None  # You can return a default value if needed, but in general this should not happen.\n",
        "\n",
        "        most_common = counter.most_common(1)[0][0]\n",
        "        return most_common\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        tree_preds = np.swapaxes(predictions, 0, 1)\n",
        "        predictions = np.array([self._most_common_label(pred) for pred in tree_preds])\n",
        "        return predictions\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6M5dbMa0jCR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, criterion=\"gini\", max_depth=30, min_samples_split=2, min_samples_leaf=1, n_features=None):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.n_features = n_features\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Validate inputs\n",
        "        if len(X) == 0 or len(y) == 0:\n",
        "            raise ValueError(\"Input data is empty\")\n",
        "        if len(X) != len(y):\n",
        "            raise ValueError(\"X and y must have the same length\")\n",
        "\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_feats = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # Check stopping criteria\n",
        "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # Feature selection\n",
        "        feat_idxs = np.random.choice(n_feats, min(self.n_features, n_feats) if self.n_features else n_feats, replace=False)\n",
        "\n",
        "        # Find best split\n",
        "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "        # If no split found, return leaf node\n",
        "        if best_feature is None or best_thresh is None:\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # Create child nodes\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
        "\n",
        "        # Check if split results in nodes with less than min_samples_leaf\n",
        "        if len(left_idxs) < self.min_samples_leaf or len(right_idxs) < self.min_samples_leaf:\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
        "        return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "    def _best_split(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "\n",
        "            for thr in thresholds:\n",
        "                # Calculate information gain\n",
        "                gain = self._information_gain(y, X_column, thr)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    left_idxs, right_idxs = self._split(X_column, thr)\n",
        "                    # Only consider splits that result in both children having at least min_samples_leaf\n",
        "                    if len(left_idxs) >= self.min_samples_leaf and len(right_idxs) >= self.min_samples_leaf:\n",
        "                        best_gain = gain\n",
        "                        split_idx = feat_idx\n",
        "                        split_threshold = thr\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "        # Parent entropy\n",
        "        parent_entropy = self._entropy(y)\n",
        "\n",
        "        # Create children\n",
        "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        # Calculate weighted avg entropy of children\n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
        "\n",
        "        # Calculate IG\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        hist = np.bincount(y)\n",
        "        ps = hist / len(y)\n",
        "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        if len(y) == 0:\n",
        "            return 0  # Return default value instead of raising error\n",
        "        counter = Counter(y)\n",
        "        return counter.most_common(1)[0][0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees=100, criterion=\"gini\", max_depth=30, min_samples_split=2, min_samples_leaf=1, n_features=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.n_features = n_features\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Validate inputs\n",
        "        if len(X) == 0 or len(y) == 0:\n",
        "            raise ValueError(\"Input data is empty\")\n",
        "        if len(X) != len(y):\n",
        "            raise ValueError(\"X and y must have the same length\")\n",
        "\n",
        "        self.trees = []\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        # Calculate number of features to use\n",
        "        if isinstance(self.n_features, str):\n",
        "            if self.n_features == 'sqrt':\n",
        "                n_feature = int(math.sqrt(n_features))\n",
        "            elif self.n_features == 'log2':\n",
        "                n_feature = int(math.log2(n_features))\n",
        "            else:\n",
        "                n_feature = None\n",
        "        else:\n",
        "            n_feature = self.n_features\n",
        "\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(\n",
        "                criterion=self.criterion,\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                min_samples_leaf=self.min_samples_leaf,\n",
        "                n_features=n_feature\n",
        "            )\n",
        "            X_sample, y_sample = self._bootstrap_samples(X, y)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def _bootstrap_samples(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        return X[idxs], y[idxs]\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        if len(y) == 0:\n",
        "            return 0  # Return default value instead of raising error\n",
        "        counter = Counter(y)\n",
        "        return counter.most_common(1)[0][0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        tree_preds = np.swapaxes(predictions, 0, 1)\n",
        "        return np.array([self._most_common_label(pred) for pred in tree_preds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "HHGpT7R7O6q2",
        "outputId": "fd3f0b21-f549-45de-cc2e-f399d35ba2c7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a865ccea-340b-426c-80fd-0defeaf7293a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a865ccea-340b-426c-80fd-0defeaf7293a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving final_final_fr_fr.csv to final_final_fr_fr.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brHLjVDgKTub",
        "outputId": "557fa4f1-78e2-4016-8d00-1fe60e2388f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique values in id: [716549 715742 727333 606874 505318 605771 606929 609789 644623 626792\n",
            " 714863 602805 602526 497421 389328 289931 620148 387155 516975 630825\n",
            " 614043 521125 627780 492744 618747 607815 601609 602657 616449 501893\n",
            " 507448 606556 724326 616945 630962 715814 502799 605278 388888 519548\n",
            " 405521 607410 715795 625326 627220 634964 722500 602824 618355 611097\n",
            " 634328 607160 728522 718570 714598 601950 612478 501416 629710 502610\n",
            " 603476 600957 713423 722431 633234 148179 715845 632153 606552 738409\n",
            " 612096 743537 632896 605207 617542 719554 627427 411315 631507 627798\n",
            " 712093 627488 626493 601363 712898 603398 631012 601848 612131 408630\n",
            " 632091 601122 602871 520828 514199 389697 714430 612567 744468 502722\n",
            " 519681 631125 606119 627252 616568 603838 719735 608062 608704 520383\n",
            " 603793 603933 605519 524803 629712 627505 526131 607065 500077 633350\n",
            " 601945 629878 712766 603664 840827 604897 525422 602560 738379 622023\n",
            " 604770 608318 626757 745297 627599 520037 632189 627636 295428 228777\n",
            " 609278 614089 607517 714607 716650 497695 496858 608688 604386 628559\n",
            " 715152 604110 629010 605254 628990 625295 603415 635533 630389 714901\n",
            " 629792 629025 524441 389109 631216 738875 634669 409940 399605 721831\n",
            " 835223 712938 395712 603388 617774 289440 714503 714893 601094 618150\n",
            " 607944 629844 626482 609257 717921 181851 629041 604062 716749 633143\n",
            " 743828 520861 517019 626779 611716 193195 627530 626479 624540 625315\n",
            " 601941 723968 715806 720045 713585 611188 606726 629142 634825 613409\n",
            " 633224 738850 500470  13183 627169 229041 496344 409723 630949 413014\n",
            " 310438 627430 526510 390823 717116 712143 612538 627539 603812 740568\n",
            " 712702 519638 627800 406863 726962 628077 601693 517096 606821 455047\n",
            " 625576 747734 517064 387330 746750 629285 743488 523710 635179 313562\n",
            " 712670 635627 524485 632320 633520 606905 393424 618333 628558 613776\n",
            " 317214 628093 411781 525567 634455 498848 638838 411253 410165 608554\n",
            " 602394 743623  87783 632431 744091 519570 744484 508149 524992 413570\n",
            " 306207 634468 522205   5978 742611 631198 606150 313577 629700 630928\n",
            " 607286 607609 522890 630227 603607 617029 617462 633001 634690 630198\n",
            " 520507 635065 613209 605452  24137 518809 634646 508156 632909 414734\n",
            " 630250 411810 214145 628986 211158 507382 522877 635024 602845 519822\n",
            " 600849 605148 631600 716142 525853 835124 634764 630117 631531 714062\n",
            " 629156 498563 625319 723931 526497 630999  97339 631909 625349 632299\n",
            " 616634 616836 609116 626789 504599 632684 522967 632893 632156 603582\n",
            " 626754 298481 631583 718026 616713 602533 522345 629758 631496 491374\n",
            " 632051 409930 632975 407221 204508 618700 627792 712054 631881 716188\n",
            " 628744 604132 629807 741428 633024 631221 601983 603007 601217 629221\n",
            " 634705 630070 630006 629072 630044 626026 520303 630972 630948 626778\n",
            " 631181 633541 630098 629959 633097 605290 629869 632868 209691 600915\n",
            " 525420 414710 629985 523128 727410 616457 206487  68949 419835 745732\n",
            " 543739 410380 385307 634566 625818 631164 603130 229211 626849 634650\n",
            " 632125 626812 631109 716766 494103 628102 634683 517774 408898 603189\n",
            " 606857 523080 631014 503957 524890 717411 525311 630967 604969 524480\n",
            " 630121 629615 634977 630823 632228 504548 522477 523461 312640 854745\n",
            " 711854 739254 621555 602482 630416 505458 296589 604826 387820 603612\n",
            " 712252 742226 712771 746466 601737 631902 312320 519986 626771 407257\n",
            " 626883 727339 631448 634885 619932 601986 631141 631069 634508 206744\n",
            " 498213 627496 629659 630190 401586 629263 631678 411681 518733 630378\n",
            " 631142 519976 604991 522483 629723 387522 613484 388729 494373 630977\n",
            " 621982 605537 607073 624443 718089 622727 609617 526064 609179 627638\n",
            " 411117 634430 631711 606010 631436 633712 609556 520090 602234 741592\n",
            " 628547 501513 630929 617379 617310 495424 622787 631716 741658 620745\n",
            " 611932 635602 619892 529289 630738 524766 522148 601547 631046  66222\n",
            " 603182 632117 630209 413395 386750 617178 630128 628736]\n",
            "Unique values in Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex): [3 4 2 5 1]\n",
            "Unique values in Q2: How many ingredients would you expect this food item to contain?: ['6' '2' '5' '3' '4' '9' '10' '8' '7' '21' '11' '15' '1' '13' '12' 'none'\n",
            " '20' '25' '17' '14' '18' '23']\n",
            "Unique values in Q3: In what setting would you expect this food to be served? Please check all that apply: ['Week day lunch,At a party,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,Weekend dinner'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,Weekend dinner,At a party,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,Weekend dinner,At a party'\n",
            " 'Week day lunch'\n",
            " 'Week day dinner,Weekend lunch,Weekend dinner,At a party,Late night snack'\n",
            " 'Weekend dinner,At a party' 'Weekend lunch,Weekend dinner,At a party'\n",
            " 'Weekend lunch,Weekend dinner,At a party,Late night snack'\n",
            " 'Week day dinner,At a party'\n",
            " 'Week day dinner,Weekend dinner,At a party,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend dinner,At a party'\n",
            " 'Weekend dinner,At a party,Late night snack' 'At a party'\n",
            " 'Week day lunch,Weekend lunch,At a party'\n",
            " 'Week day lunch,Week day dinner,At a party'\n",
            " 'Week day lunch,Weekend dinner,At a party,Late night snack'\n",
            " 'Week day dinner,At a party,Late night snack'\n",
            " 'Week day dinner,Weekend dinner,At a party'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,At a party'\n",
            " 'Week day lunch,Week day dinner,At a party,Late night snack'\n",
            " 'Weekend lunch,At a party,Late night snack'\n",
            " 'Week day lunch,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,At a party,Late night snack'\n",
            " 'Week day lunch,Weekend lunch,At a party,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend dinner,At a party,Late night snack'\n",
            " 'Week day lunch,At a party' 'Week day lunch,Weekend lunch'\n",
            " 'Week day dinner,Weekend lunch,Weekend dinner,At a party'\n",
            " 'Weekend lunch,At a party' 'Weekend lunch,Weekend dinner'\n",
            " 'Week day lunch,Weekend dinner,At a party' 'Week day dinner'\n",
            " 'Week day lunch,Weekend lunch,Weekend dinner,At a party,Late night snack'\n",
            " 'Week day dinner,Weekend dinner'\n",
            " 'Week day dinner,Weekend lunch,At a party,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Late night snack'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch,Weekend dinner,Late night snack'\n",
            " 'Weekend lunch' 'Week day lunch,Weekend lunch,Late night snack'\n",
            " 'Week day lunch,Week day dinner'\n",
            " 'Week day lunch,Week day dinner,Weekend lunch'\n",
            " 'Week day lunch,Weekend dinner'\n",
            " 'Week day dinner,Weekend dinner,Late night snack'\n",
            " 'Week day lunch,Weekend dinner,Late night snack'\n",
            " 'Weekend dinner,Late night snack'\n",
            " 'Week day lunch,Weekend lunch,Weekend dinner'\n",
            " 'Week day lunch,Weekend lunch,Weekend dinner,Late night snack'\n",
            " 'Week day dinner,Weekend lunch,Weekend dinner'\n",
            " 'Week day lunch,Week day dinner,Weekend dinner' 'Weekend dinner'\n",
            " 'Week day lunch,Weekend lunch,Weekend dinner,At a party'\n",
            " 'Week day dinner,Weekend lunch,At a party' 'At a party,Late night snack'\n",
            " 'Week day dinner,Weekend lunch' 'Week day dinner,Late night snack'\n",
            " 'Late night snack' 'Weekend lunch,Weekend dinner,Late night snack'\n",
            " 'Week day dinner,Weekend lunch,Weekend dinner,Late night snack']\n",
            "Unique values in Q4: How much would you expect to pay for one serving of this food item?: ['5.0' '10.0' '3.0' '15.0' '1.0' '20.0' '4.0' '12.0' '6.0' '30.0' '7.0'\n",
            " '8.0' '22.5' '11.5' '13.0' '6.5' '35.0' '4.99' '18.0' '2.0' '2.5' '3.5'\n",
            " '17.0' '22.0' '5.5' '16.0' '25.0' '14.0' '3.75' '4.5' '2.29' '64.0' '7.5'\n",
            " '9.0' '8.5' '11.0' '12.5' 'none' '5.99' '16.99' '57.0' '7.25' '65.0'\n",
            " '23.0' '50.0' '17.5' '45.0' '13.5' '80.0' '28.0' '14.5' '6.96' '62.0'\n",
            " '14.99' '4.29' '19.0' '16.5' '24.0' '3.95' '3.45' '4.25' '2.99' '3.99'\n",
            " '18.5' '11.99' '9.99' '16.98' '9.5' '10.5' '12.36' '13.99' '100.0' '1.5'\n",
            " '40.0' '20.99' '10.99' '12.79' '27.5' '11.02' '1.49' '32.5']\n",
            "Unique values in Q5: What movie do you think of when thinking of this food item?: ['cloudy with a chance of meatballs'\n",
            " 'all sort of american young boy movies' 'action movie' 'mamma mia' 'none'\n",
            " 'dragon' 'rick and morty' 'home alone' 'the big lebowski' 'spiderman'\n",
            " 'goodfellas' 'a goofy movie the disney movie' 'dead silence' 'la la land'\n",
            " 'harry potter' 'transformer' 'teenage mutant ninja turtles'\n",
            " 'high school musical 3' 'despicable me' 'toy story' 'the godfather'\n",
            " 'pizza' 'fast furious' 'garfield' 'spiderman no way home' 'ratatouille'\n",
            " 'five nights at freddys' 'the avengers' 'back to the future' 'godfather'\n",
            " 'mystic pizza' 'futurama' 'iron man' 'life of pi' 'any american movie'\n",
            " 'king kong' 'grown ups' 'deadpool' 'cloudy with a chance of meatball'\n",
            " 'whiplash' 'inside out' 'superbad' 'stranger things' 'air bud'\n",
            " 'a quiet place day one 2024' 'finding nemo' 'my cousin vinny'\n",
            " 'eat pray love' 'comedy' 'star wars' 'the dark knight' 'pulp fiction'\n",
            " 'fast and furious' 'the truman show' 'breaking bad' 'interstellar'\n",
            " 'rush hour' 'shrek' 'scooby doo' 'back to the future 2'\n",
            " 'princess diaries the apology scene' 'walle' 'spy kids 3d' 'squid game'\n",
            " 'running man' 'recep ivedik 2' 'venom' 'moneyball'\n",
            " 'hangover or any teen party movies which features a lot of sweet stuff'\n",
            " 'coraline' 'life if pi' 'james bond 007' 'the goofy movie'\n",
            " 'a quiet place 2' 'carryon' 'set it up' 'green book' 'cars'\n",
            " 'relaxing comedy' 'high school musical' 'zootopia' 'mario movie'\n",
            " '30 minutes or less' '21 jump street' 'free guy' 'harry poter' 'soul'\n",
            " 'life is beautiful' 'the wicked' 'harold kumar go to white castle'\n",
            " 'lord of the rings' 'the hangover' 'breaking bad not a movie'\n",
            " 'little italy' 'everything everywhere all at once' 'barbie' 'hitman'\n",
            " 'liquorice pizza' 'the dictator' 'anjaana anjaani' 'hawkeye' 'argo'\n",
            " 'transformers' 'aladdin' 'the invisible guest' 'titanic' '1001 nights'\n",
            " 'shawarma legend' 'dangal' 'spiderman into the spiderverse'\n",
            " 'slumdog millionaire' 'cleopatra' 'the kite runner' '2012' 'the boys'\n",
            " 'mission impossible' 'black hawk down' 'dictator' 'kungfu panda'\n",
            " 'alladin' 'how to lose a guy in 10 days' 'babylon' 'prince of egypt'\n",
            " 'borat' 'harold and kumar' 'gossip girl' 'monty python'\n",
            " 'khabi khushi khabi gham' 'mission impossible dead reckoning part one'\n",
            " '3 idiots' 'dune' 'parasite' 'batman the dark knight' 'the mummy'\n",
            " 'taxi driver' 'comedy movie' 'you dont mess with the zohan'\n",
            " 'spider man spider verse' 'gladiator' 'any bollywood movie'\n",
            " 'scott pilgrim vs the world' 'son of babylon'\n",
            " 'cloudy with a chance of meatballs again shawarmas dont appear in that though but hey its food'\n",
            " 'bad boys' 'the social network' 'documenary' 'chef' 'minions'\n",
            " 'the big sick' 'jurassic park' 'wicked' 'the avengers 1' 'the avenger'\n",
            " 'arcane' 'bahen' 'legend of shawama' 'forest gump' 'john wick'\n",
            " 'scott pilgrim' 'good will hunting' 'fight club' 'shawarma bowl' 'drive'\n",
            " 'lion king' 'captain of america' 'spirited away' 'japanese movies'\n",
            " 'cartoon movie' 'johnny english' 'karate kid' 'drange' 'kung fu panda'\n",
            " 'wolverine' 'jiro dreams of sushi' 'kill bill' 'your name'\n",
            " 'once upon a time in hollywood'\n",
            " 'the lord of the rings the fellowship of the ring' 'shark tale'\n",
            " 'bullet train' 'japanese' 'isle of dogs' 'little forest a japanese film'\n",
            " 'doraemon' 'naruto' 'jaws' 'the end of evangelion' 'midnight diner'\n",
            " 'anime' 'crazy rich asians' 'the perfect storm' 'the karate kid'\n",
            " 'the pacific' 'weathering with you'\n",
            " 'eternal sunshine of the spotless mind' 'cars 2' 'my neighbour totoro'\n",
            " 'oldboy' 'the proposal' 'rurouni kenshin' 'gone girl' 'my hero academia'\n",
            " 'food wars' 'your name 2016' 'liz and the blue bird' 'spirited away 2001'\n",
            " 'frozen' 'seven samurai' 'the boy and the heron' 'howls moving castle'\n",
            " 'suits' 'john wick 4' 'ghibli movies' 'bladerunner' 'a silent voice 2016'\n",
            " 'monsters inc' 'jiro dreams of sush' 'inception' 'big big wolf'\n",
            " 'wolf of wall street' 'romance movie' 'wolf of wallstreet' 'godzilla'\n",
            " 'scary movie 4 2006' 'monster house' 'the big short' 'the little mermaid'\n",
            " 'lost in translation' 'pirates of the carribeans'\n",
            " 'jiro dreams of sushi east side sushi finding nemo' 'monster inc'\n",
            " 'spirit away' 'gran turismo' 'short movie because i eat it fast'\n",
            " 'the breakfast club' 'ponyo' 'samurai jack' 'mean girls' 'girls trip'\n",
            " 'my neighbor totoro' 'anime i guess' 'passengers2016' 'madagascar'\n",
            " 'koe no katachi' 'talented' 'crayon shinchan the movie'\n",
            " 'memoirs of a geisha' 'masterchef' 'monster' 'the last samurai' 'suzume'\n",
            " 'detective conan' 'big hero 6' 'snowpiercer' 'yakuza'\n",
            " 'wizards of waverly place movie'\n",
            " 'teenage mutant ninja turtles the turtles are famous for their love of pizza'\n",
            " 'spiderman across the spiderverse' 'saving private ryan' 'mr deeds'\n",
            " 'documentary' 'murder mystery' 'the super mario bros movie' 'the ritual'\n",
            " 'breakaway' 'uncle grandpa' 'captain america' 'pirates of the caribbean'\n",
            " 'the boy and the herron' 'the grinch' 'the lego movie'\n",
            " 'futurama benders big score' 'the italian job'\n",
            " 'back to the future part 2'\n",
            " 'any kind really probably something more relaxed for leisure'\n",
            " 'american pie' 'friday' 'this is the end'\n",
            " 'ready player one there is pacman inside and pizza looks like one'\n",
            " 'the gentlemen' 'gilmore girls' 'happy gilmore' 'ratatoullie'\n",
            " 'back in action' 'die hard' 'neverending story' 'maze runner'\n",
            " 'diary of a wimpy kid rodrick rules' 'malena' 'anchorman' 'friends'\n",
            " 'terminator' 'men in black' 'the davinci code' 'crazy stupid love'\n",
            " 'the hunger games' 'luca' 'any movie set in new york' 'cartoons'\n",
            " 'waynes world' 'scific type' 'do the right thing' 'space jam' 'spongebob'\n",
            " 'diary of a wimpy kid' 'step brothers' 'horror movie'\n",
            " 'the princess diaries' 'oppenheimer'\n",
            " 'i think of movies that ive watch with either my friends or my siblings'\n",
            " 'the barbie movie' 'pursuit of happyness' 'middle east movie' 'drishyam'\n",
            " 'us' 'shrek2' 'nosferatu' 'alien' 'indiana jones' 'angry birds'\n",
            " 'the road to fallujah' 'kingdom of heaven' 'lawrence of arabia'\n",
            " 'time of happiness' 'middle eastern movies' 'hangover' 'coco' '11sep'\n",
            " '13 hours' 'murder on the orient express' 'three idiots' 'shazam'\n",
            " 'the lion king' 'aladdin idk' 'deadpool and wolverine' 'mulan' 'mandoob'\n",
            " 'the prince of egypt' 'indian movie' 'south park the end of obesity'\n",
            " 'yeh jawaani hai deewani' 'dabba' 'cheech chong' 'shawshank redemption'\n",
            " 'mad max' 'taken 2' 'ferris buellers day off' 'the dora movie'\n",
            " 'the whale' 'chinese zodiac' 'sonic the hedgehog' 'bollywood'\n",
            " 'middle eastern movie' 'burnt' 'rush hour 3' 'scary movie' 'the menu'\n",
            " 'the emoji movie' 'rush hour 2' 'a silent voice' 'pokemon' 'house md'\n",
            " 'the fast and the furious tokyo drift' 'who am i jackie chan'\n",
            " 'japanese movies food documentaries' 'kodoku no gurume' 'gi joe'\n",
            " 'penguins of madagascar' 'japanese movie' 'shrek3' 'aquaman' '47 ronin'\n",
            " 'barbie 2021' 'pengnuins of madagascar' '9' 'james bond' 'wags'\n",
            " 'notting hill' 'made in abyss dawn of the deep soul' 'meitantei konan'\n",
            " 'shangchi' 'billions tv show' 'madagascar 2' 'ip man' 'django unchained'\n",
            " 'the samurai' 'shangchi and the legend of the ten rings' 'karate kid ii'\n",
            " '7 samurai' 'casablanca' 'pacific rim' 'train to busan' 'love hard'\n",
            " 'shogun' 'the room' 'fallen angels' 'totoro'\n",
            " 'shinchan the movie chounouryoku daikessen tobe tobe temakizushi'\n",
            " 'shanghai noon' 'one piece' 'turning red' 'blade runner'\n",
            " 'fast furious tokyo drift' 'lucy' 'hunger' 'the intern' 'the meg 2018'\n",
            " 'rush hour series' 'ultraman rising' 'pearl harbour' 'good time'\n",
            " 'who am i 2005' 'memory of a geisha' 'last samurai' 'city hunter'\n",
            " 'alitathe warrior' 'haikyu' 'the spy next door' 'shang chi'\n",
            " 'breakfast club' 'heretic' 'crayon shinchan' 'anime movie' 'cars 3'\n",
            " 'east side sushi' 'shang chi the legend of ten rings'\n",
            " 'chandni chowk to china' 'midnight dinner' 'kikis delivery service'\n",
            " 'romantic movies' 'one piece film gold']\n",
            "Unique values in Q6: What drink would you pair with this food item?: ['coke' 'soda' 'ice tea' 'lemonade' 'red wine' 'sprite' 'water' 'soup'\n",
            " 'root beer' '7up' 'none' 'orange fanta' 'mountain dew' 'diet pepsi'\n",
            " 'juice' 'dr pepper' 'soft drink' 'pop' 'ginger ale' 'milk' 'boba'\n",
            " 'lemon tea' 'ayran' 'lassi' 'carbonated drink' 'the avengers' 'jarritos'\n",
            " 'oolong tea' 'sake' 'green tea' 'calpis' 'barley tea' 'hot tea'\n",
            " 'fish sauce' 'matcha' 'champagne' 'crush' 'pineapple fanta' 'canada dry'\n",
            " 'gatorade' 'sprindrift' 'martini cocktail' 'smoothie' 'laban' 'yogurt'\n",
            " 'coffee' 'barbican' 'leban' 'fruit tea' 'dairy' 'powerade' 'ramune'\n",
            " 'mango pulp' 'diet brisk' 'yakult' 'saporo' 'soy sauce' 'baijiu'\n",
            " 'any tea']\n",
            "Unique values in Q7: When you think about this food item, who does it remind you of?: ['Friends' 'Friends,Teachers,Strangers' 'Siblings,Friends,Teachers'\n",
            " 'Siblings,Friends' 'none' 'Parents,Siblings,Friends' 'Parents,Friends'\n",
            " 'Friends,Strangers' 'Parents,Siblings,Friends,Strangers' 'Parents'\n",
            " 'Parents,Siblings,Friends,Teachers' 'Friends,Teachers' 'Teachers'\n",
            " 'Parents,Friends,Teachers' 'Siblings,Teachers' 'Teachers,Strangers'\n",
            " 'Siblings' 'Siblings,Friends,Strangers' 'Parents,Siblings,Teachers'\n",
            " 'Parents,Siblings' 'Strangers' 'Siblings,Friends,Teachers,Strangers'\n",
            " 'Parents,Friends,Teachers,Strangers'\n",
            " 'Parents,Siblings,Friends,Teachers,Strangers' 'Parents,Teachers'\n",
            " 'Parents,Friends,Strangers' 'Parents,Siblings,Strangers'\n",
            " 'Parents,Strangers' 'Siblings,Strangers' 'Parents,Teachers,Strangers']\n",
            "Unique values in Q8: How much hot sauce would you add to this food item?: ['A little (mild)' 'none' 'A moderate amount (medium)'\n",
            " 'I will have some of this food item with my hot sauce' 'A lot (hot)']\n",
            "Unique values in Label: ['Pizza' 'Shawarma' 'Sushi']\n",
            "X shape: (1644, 23), y shape: (1644,)\n",
            "Data verification:\n",
            "X dtype: float64\n",
            "X sample:\n",
            "[[3.00000000e+00 6.00000000e+00 5.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.52068127e-02 2.57299270e-01]\n",
            " [4.00000000e+00 2.00000000e+00 5.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 6.08272506e-04 2.57299270e-01]\n",
            " [3.00000000e+00 5.00000000e+00 1.00000000e+01 0.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.21654501e-03 2.57299270e-01]]\n",
            "y sample: [0 0 0 0 0 0 0 0 0 0]\n",
            "q3_hot dtype: float64\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1644 entries, 0 to 1643\n",
            "Data columns (total 3 columns):\n",
            " #   Column                                                                                                                 Non-Null Count  Dtype  \n",
            "---  ------                                                                                                                 --------------  -----  \n",
            " 0   Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)  1644 non-null   int64  \n",
            " 1   Q2: How many ingredients would you expect this food item to contain?                                                   1644 non-null   float64\n",
            " 2   Q4: How much would you expect to pay for one serving of this food item?                                                1644 non-null   float64\n",
            "dtypes: float64(2), int64(1)\n",
            "memory usage: 38.7 KB\n",
            "None\n",
            "q5_encoded contains NaN: False\n",
            "Unique values after encoding: ['Pizza' 'Shawarma' 'Sushi']\n",
            "Encoded labels: [0 0 0 0 0 0 0 0 0 0]\n",
            "Label\n",
            "Pizza       548\n",
            "Shawarma    548\n",
            "Sushi       548\n",
            "Name: count, dtype: int64\n",
            "Training labels distribution: 0    454\n",
            "2    443\n",
            "1    418\n",
            "Name: count, dtype: int64\n",
            "Testing labels distribution: 1    130\n",
            "2    105\n",
            "0     94\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.8632\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/final_final_fr_fr.csv')\n",
        "\n",
        "# Print unique values for each column\n",
        "for column in df.columns:\n",
        "    print(f\"Unique values in {column}: {df[column].unique()}\")\n",
        "\n",
        "# Your vectorized_one_hot function\n",
        "def vectorized_one_hot(answers, options, attribute_to_index):\n",
        "    num_samples = len(answers)\n",
        "    num_attributes = len(options)\n",
        "    one_hot_matrix = np.zeros((num_samples, num_attributes))\n",
        "\n",
        "    for i, ans_list in enumerate(answers):\n",
        "        for ans in ans_list:\n",
        "            ans = ans.strip()\n",
        "            if ans in attribute_to_index:\n",
        "                one_hot_matrix[i, attribute_to_index[ans]] = 1\n",
        "    return one_hot_matrix\n",
        "\n",
        "# Define question columns\n",
        "questions = [q1, q2, q3, q4, q5, q6, q7, q8] = [\n",
        "    \"Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)\",\n",
        "    \"Q2: How many ingredients would you expect this food item to contain?\",\n",
        "    \"Q3: In what setting would you expect this food to be served? Please check all that apply\",\n",
        "    \"Q4: How much would you expect to pay for one serving of this food item?\",\n",
        "    \"Q5: What movie do you think of when thinking of this food item?\",\n",
        "    \"Q6: What drink would you pair with this food item?\",\n",
        "    \"Q7: When you think about this food item, who does it remind you of?\",\n",
        "    \"Q8: How much hot sauce would you add to this food item?\"\n",
        "]\n",
        "t = 'Label'\n",
        "\n",
        "df[q1] = pd.to_numeric(df[q1], errors='coerce')\n",
        "df[q2] = pd.to_numeric(df[q2], errors='coerce')\n",
        "df[q4] = pd.to_numeric(df[q4], errors='coerce')\n",
        "\n",
        "# Create missing indicators before imputation\n",
        "for col in [q1, q2, q4]:\n",
        "    df[f'{col}_missing'] = df[col].isna().astype(int)\n",
        "\n",
        "# # Fill numerical NaNs with median (now including missing indicators)\n",
        "# df[q1].fillna(df[q1].median(), inplace=True)\n",
        "# df[q2].fillna(df[q2].median(), inplace=True)\n",
        "# df[q4].fillna(df[q4].median(), inplace=True)\n",
        "\n",
        "\n",
        "# 1. Fix pandas warnings\n",
        "df = df.fillna({\n",
        "    q1: df[q1].median(),\n",
        "    q2: df[q2].median(),\n",
        "    q4: df[q4].median()\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# Define options for categorical columns\n",
        "q3_options = ['none','Week day lunch','Week day dinner','Weekend lunch',\n",
        "             'Weekend dinner','At a party', 'Late night snack']\n",
        "q7_options = ['Parents','Siblings','Friends', 'Teachers', 'Strangers', 'none']\n",
        "q8_options = ['I will have some of this food item with my hot sauce',\n",
        "             'A lot (hot)', 'A moderate amount (medium)', 'A little (mild)', 'none']\n",
        "\n",
        "# Create attribute to index mappings\n",
        "q3_attribute_to_index = {attr: idx for idx, attr in enumerate(q3_options)}\n",
        "q7_attribute_to_index = {attr: idx for idx, attr in enumerate(q7_options)}\n",
        "q8_attribute_to_index = {attr: idx for idx, attr in enumerate(q8_options)}\n",
        "\n",
        "# Manual feature engineering approach\n",
        "numerical_features = df[[q1, q2, q4]].values\n",
        "\n",
        "# Convert DataFrame columns to lists before splitting\n",
        "q3_answers = [ans.split(\",\") for ans in df[q3].astype(str).tolist()]\n",
        "q7_answers = [ans.split(\",\") for ans in df[q7].astype(str).tolist()]\n",
        "q8_answers = [ans.split(\",\") for ans in df[q8].astype(str).tolist()]\n",
        "\n",
        "q3_hot = vectorized_one_hot(q3_answers, q3_options, q3_attribute_to_index)\n",
        "q7_hot = vectorized_one_hot(q7_answers, q7_options, q7_attribute_to_index)\n",
        "q8_hot = vectorized_one_hot(q8_answers, q8_options, q8_attribute_to_index)\n",
        "\n",
        "# Frequency encoding\n",
        "q5_encoded = df[q5].map(df[q5].value_counts(normalize=True)).values.reshape(-1, 1)\n",
        "q6_encoded = df[q6].map(df[q6].value_counts(normalize=True)).values.reshape(-1, 1)\n",
        "\n",
        "# # Combine features\n",
        "# X = np.hstack([\n",
        "#     numerical_features,\n",
        "#     q3_hot,\n",
        "#     q7_hot,\n",
        "#     q8_hot,\n",
        "#     q5_encoded,\n",
        "#     q6_encoded\n",
        "# ])\n",
        "\n",
        "# 2. Ensure numerical features\n",
        "X = np.hstack([\n",
        "    numerical_features.astype(float),\n",
        "    q3_hot.astype(float),\n",
        "    q7_hot.astype(float),\n",
        "    q8_hot.astype(float),\n",
        "    q5_encoded.astype(float),\n",
        "    q6_encoded.astype(float)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[t])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# 3. Verify before training\n",
        "assert X.dtype.kind in ('f', 'i'), \"X contains non-numeric values\"\n",
        "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Before training, verify your data\n",
        "print(\"Data verification:\")\n",
        "print(f\"X dtype: {X.dtype}\")\n",
        "print(f\"X sample:\\n{X[:3]}\")\n",
        "print(f\"y sample: {y[:10]}\")\n",
        "\n",
        "# Check for any remaining strings/objects\n",
        "assert X.dtype.kind in ('f', 'i'), f\"X contains non-numeric values: {X.dtype}\"\n",
        "assert y.dtype.kind in ('i'), f\"y contains non-integer values: {y.dtype}\"\n",
        "\n",
        "print(f\"q3_hot dtype: {q3_hot.dtype}\")\n",
        "print(df[[q1, q2, q4]].info())\n",
        "print(f\"q5_encoded contains NaN: {np.isnan(q5_encoded).any()}\")\n",
        "\n",
        "\n",
        "print(f\"Unique values after encoding: {le.classes_}\")\n",
        "print(f\"Encoded labels: {y[:10]}\")  # Print first 10 encoded labels\n",
        "print(df[t].value_counts())  # Get counts for each class in the target column\n",
        "print(f\"Training labels distribution: {pd.Series(y_train).value_counts()}\")\n",
        "print(f\"Testing labels distribution: {pd.Series(y_test).value_counts()}\")\n",
        "\n",
        "\n",
        "# Initialize with proper parameters\n",
        "rf_model = RandomForest(\n",
        "    n_trees=100,\n",
        "    criterion='gini',\n",
        "    max_depth=30,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=1,\n",
        "    n_features='sqrt'\n",
        ")\n",
        "\n",
        "# Train and predict\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.sum(y_true == y_pred) / len(y_true)\n",
        "\n",
        "acc = accuracy(y_test, predictions)\n",
        "print(f\"Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "esp3ghNTPVEa"
      },
      "outputs": [],
      "source": [
        "def build_all_models(n_estimators,\n",
        "                     criterion,\n",
        "                     max_depths,\n",
        "                     min_samples_split,\n",
        "                     min_samples_leaf,\n",
        "                     X_train=X_train,\n",
        "                     y_train=y_train,\n",
        "                     X_test=X_test,\n",
        "                     y_test=y_test):\n",
        "    \"\"\"\n",
        "     min_samples_leaf\n",
        "\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for n in n_estimators:\n",
        "      for d in max_depths:\n",
        "        for s in min_samples_split:\n",
        "            for l in min_samples_leaf:\n",
        "                out[(n, d, s, l)] = {}\n",
        "\n",
        "                rf_model = RandomForest(\n",
        "                              n_trees=n,\n",
        "                              criterion=criterion,\n",
        "                              max_depth=d,\n",
        "                              min_samples_split=s,\n",
        "                              min_samples_leaf=l,\n",
        "                              n_features='sqrt'\n",
        "                          )\n",
        "\n",
        "                rf_model.fit(X_train, y_train)\n",
        "\n",
        "                predictions = rf_model.predict(X_test)\n",
        "\n",
        "                # TODO: store the validation and training scores in the `out` dictionary\n",
        "                out[(n, d, s, l)]['test'] = accuracy(y_test, predictions) # TODO\n",
        "\n",
        "                # accuracy(y_test, predictions)\n",
        "                # rf_model.score(X_train, y_train) # TODO\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-o9lJGaQ1_a",
        "outputId": "62395ec5-7f90-4708-efc6-b314474c465b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using criterion entropy\n",
            "Best n_trees:  100 Best depth:  50 . Best samples:  1 Best leaf:  2 . Accuracy:  0.8693009118541033\n",
            "\n",
            "Using criterion gini\n",
            "Best n_trees:  100 Best depth:  50 . Best samples:  1 Best leaf:  1 . Accuracy:  0.8723404255319149\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters values to try in our grid search\n",
        "n_estimators = [50, 100]\n",
        "criterions = [\"entropy\", \"gini\"]\n",
        "max_depths = [1, 10, 50]\n",
        "min_samples_split = [1, 2, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "rf_model = RandomForest(\n",
        "    n_trees=100,\n",
        "    criterion='gini',\n",
        "    max_depth=30,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=1,\n",
        "    n_features='sqrt'\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "for criterion in criterions:\n",
        "    print(\"\\nUsing criterion {}\".format(criterion))\n",
        "    res = build_all_models(n_estimators, criterion, max_depths, min_samples_split, min_samples_leaf)\n",
        "\n",
        "    best_acc = 0\n",
        "    best_crit = None\n",
        "    for (n, d, s, l) in res:\n",
        "      if res[(n, d, s, l)]['test'] > best_acc:\n",
        "        best_acc = res[(n, d, s, l)]['test']\n",
        "        best_crit = (n, d, s, l)\n",
        "    print(\"Best n_trees: \", best_crit[0], \"Best depth: \", best_crit[1], \". Best samples: \", best_crit[2], \"Best leaf: \", best_crit[3], \". Accuracy: \", best_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters values to try in our grid search\n",
        "n_estimators = [50, 100]\n",
        "criterions = [\"entropy\", \"gini\"]\n",
        "max_depths = [20, 30, 50]\n",
        "min_samples_split = [1, 10, 15]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "rf_model = RandomForest(\n",
        "    n_trees=100,\n",
        "    criterion='gini',\n",
        "    max_depth=30,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=1,\n",
        "    n_features='sqrt'\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "for criterion in criterions:\n",
        "    print(\"\\nUsing criterion {}\".format(criterion))\n",
        "    res = build_all_models(n_estimators, criterion, max_depths, min_samples_split, min_samples_leaf)\n",
        "\n",
        "    best_acc = 0\n",
        "    best_crit = None\n",
        "    for (n, d, s, l) in res:\n",
        "      if res[(n, d, s, l)]['test'] > best_acc:\n",
        "        best_acc = res[(n, d, s, l)]['test']\n",
        "        best_crit = (n, d, s, l)\n",
        "    print(\"Best n_trees: \", best_crit[0], \"Best depth: \", best_crit[1], \". Best samples: \", best_crit[2], \"Best leaf: \", best_crit[3], \". Accuracy: \", best_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-xGu9On8K9t",
        "outputId": "8cfcfec7-f84c-43ac-8977-d8076ab61cf1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using criterion entropy\n",
            "Best n_trees:  100 Best depth:  30 . Best samples:  10 Best leaf:  2 . Accuracy:  0.878419452887538\n",
            "\n",
            "Using criterion gini\n",
            "Best n_trees:  100 Best depth:  30 . Best samples:  1 Best leaf:  2 . Accuracy:  0.8723404255319149\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}